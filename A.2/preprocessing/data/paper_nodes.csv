paper_id,title,year,abstract,doi
033275ccc2c7c5c38592ae893da0b5923cf90717,Neural Graph Reasoning: Complex Logical Query Answering Meets Graph Databases,2023,"Complex logical query answering (CLQA) is a recently emerged task of graph machine learning that goes beyond simple one-hop link prediction and solves a far more complex task of multi-hop logical reasoning over massive, potentially incomplete graphs in a latent space. The task received a significant traction in the community; numerous works expanded the field along theoretical and practical axes to tackle different types of complex queries and graph modalities with efficient systems. In this paper, we provide a holistic survey of CLQA with a detailed taxonomy studying the field from multiple angles, including graph types (modality, reasoning domain, background semantics), modeling aspects (encoder, processor, decoder), supported queries (operators, patterns, projected variables), datasets, evaluation metrics, and applications. Refining the CLQA task, we introduce the concept of Neural Graph Databases (NGDBs). Extending the idea of graph databases (graph DBs), NGDB consists of a Neural Graph Storage and a Neural Graph Engine. Inside Neural Graph Storage, we design a graph store, a feature store, and further embed information in a latent embedding store using an encoder. Given a query, Neural Query Engine learns how to perform query planning and execution in order to efficiently retrieve the correct results by interacting with the Neural Graph Storage. Compared with traditional graph DBs, NGDBs allow for a flexible and unified modeling of features in diverse modalities using the embedding store. Moreover, when the graph is incomplete, they can provide robust retrieval of answers which a normal graph DB cannot recover. Finally, we point out promising directions, unsolved problems and applications of NGDB for future research.",10.48550/arXiv.2303.14617
459cf4e6476d5e19f1bc422c162bcded54ca7f95,"Experimental Evaluation of Graph Databases: JanusGraph, Nebula Graph, Neo4j, and TigerGraph",2023,"NoSQL databases were created with the primary goal of addressing the shortcomings in the efficiency of relational databases, and can be of four types: document, column, key-value, and graph databases. Graph databases can store data and relationships efficiently, and have a flexible and easy-to-understand data schema. In this paper, we perform an experimental evaluation of the four most popular graph databases: JanusGraph, Nebula Graph, Neo4j, and TigerGraph. Database performance is evaluated using the Linked Data Benchmark Council’s Social Network Benchmark (LDBC SNB). In the experiments, we analyze the execution time of the queries, the loading time of the nodes and the RAM and CPU usage for each database. In our analysis, Neo4j was the graph database with the best performance across all metrics.",10.3390/app13095770
4340043a5a43f1863e080e3fc717f863ad21b51d,Graph Databases,2023,,10.1201/9781003183532
1a85628dd645ea86b7c146613651fdb30bb32c1d,The World of Graph Databases from An Industry Perspective,2022,"Rapidly growing social networks and other graph data have created a high demand for graph technologies in the market. A plethora of graph databases, systems, and solutions have emerged, as a result. On the other hand, graph has long been a well studied area in the database research community. Despite the numerous surveys on various graph research topics, there is a lack of survey on graph technologies from an industry perspective. The purpose of this paper is to provide the research community with an industrial perspective on the graph database landscape, so that graph researcher can better understand the industry trend and the challenges that the industry is facing, and work on solutions to help address these problems.",10.1145/3582302.3582320
5abf79226a8ffa2dcf33ca98bfc36c8bb01cc4fc,Multilayer graphs: a unified data model for graph databases,2022,"In this short position paper, we argue that there is a need for a unifying data model that can support popular graph formats such as RDF, RDF* and property graphs, while at the same time being powerful enough to naturally store information from complex knowledge graphs, such as Wikidata, without the need for a complex reification scheme. Our proposal, called the multilayer graph model, presents a simple and flexible data model for graphs that can naturally support all of the above, and more. We also observe that the idea of multilayer graphs has appeared in existing graph systems from different vendors and research groups, illustrating its versatility.",10.1145/3534540.3534696
59078a26c42e8609244f1c3e01f05af74a3773bf,Inference of Shape Graphs for Graph Databases,2022,"We investigate the problem of constructing a shape graph that describes the structure of a given graph database. We employ the framework of grammatical inference , where the objective is to find an inference algorithm that is both sound , i.e., always producing a schema that validates the input graph, and complete , i.e., able to produce any schema, within a given class of schemas, provided that a sufficiently informative input graph is presented. We identify a number of fundamental limitations that preclude feasible inference. We present inference algorithms based on natural approaches that allow to infer schemas that we argue to be of practical importance.",10.4230/LIPIcs.ICDT.2022.14
1cbbe1a129c0d1f53d8e16ca74238334372507dc,Neural Graph Databases,2022,"Graph databases (GDBs) enable processing and analysis of unstructured, complex, rich, and usually vast graph datasets. Despite the large significance of GDBs in both academia and industry, little effort has been made into integrating them with the predictive power of graph neural networks (GNNs). In this work, we show how to seamlessly combine nearly any GNN model with the computational capabilities of GDBs. For this, we observe that the majority of these systems are based on, or support, a graph data model called the Labeled Property Graph (LPG), where vertices and edges can have arbitrarily complex sets of labels and properties. We then develop LPG2vec, an encoder that transforms an arbitrary LPG dataset into a representation that can be directly used with a broad class of GNNs, including convolutional, attentional, message-passing, and even higher-order or spectral models. In our evaluation, we show that the rich information represented as LPG labels and properties is properly preserved by LPG2vec, and it increases the accuracy of predictions regardless of the targeted learning task or the used GNN model, by up to 34% compared to graphs with no LPG labels/properties. In general, LPG2vec enables combining predictive power of the most powerful GNNs with the full scope of information encoded in the LPG model, paving the way for neural graph databases, a class of systems where the vast complexity of maintained data will benefit from modern and future graph machine learning methods.",10.48550/arXiv.2209.09732
86e9b0585c8b866665de18b2744d69aefff3e16d,A model and query language for temporal graph databases,2021,,10.1007/s00778-021-00675-4
a20e707395132f750fa958ebc0ea629b23b57bfe,An overview of graph databases and their applications in the biomedical domain,2021,"Abstract Over the past couple of decades, the explosion of densely interconnected data has stimulated the research, development and adoption of graph database technologies. From early graph models to more recent native graph databases, the landscape of implementations has evolved to cover enterprise-ready requirements. Because of the interconnected nature of its data, the biomedical domain has been one of the early adopters of graph databases, enabling more natural representation models and better data integration workflows, exploration and analysis facilities. In this work, we survey the literature to explore the evolution, performance and how the most recent graph database solutions are applied in the biomedical domain, compiling a great variety of use cases. With this evidence, we conclude that the available graph database management systems are fit to support data-intensive, integrative applications, targeted at both basic research and exploratory tasks closer to the clinic.",10.1093/database/baab026
d852660e15e82ae150dcde105639ba2c7d90f01f,Graph Databases,2019,,10.1007/978-3-319-77525-8_100147
703d49a335799262e474ed8fe6044cd641a9e00a,Multi-Dimensional Event Data in Graph Databases,2020,"Process event data is usually stored either in a sequential process event log or in a relational database. While the sequential, single-dimensional nature of event logs aids querying for (sub)sequences of events based on temporal relations such as “directly/eventually-follows,” it does not support querying multi-dimensional event data of multiple related entities. Relational databases allow storing multi-dimensional event data, but existing query languages do not support querying for sequences or paths of events in terms of temporal relations. In this paper, we propose a general data model for multi-dimensional event data based on labeled property graphs that allows storing structural and temporal relations in a single, integrated graph-based data structure in a systematic way. We provide semantics for all concepts of our data model, and generic queries for modeling event data over multiple entities that interact synchronously and asynchronously. The queries allow for efficiently converting large real-life event data sets into our data model, and we provide 5 converted data sets for further research. We show that typical and advanced queries for retrieving and aggregating such multi-dimensional event data can be formulated and executed efficiently in the existing query language Cypher, giving rise to several new research questions. Specifically, aggregation queries on our data model enable process mining over multiple inter-related entities using off-the-shelf technology.",10.1007/s13740-021-00122-1
2d95c27d4ed8cfb8379164888cd79c6b239250b4,Mapping RDF Databases to Property Graph Databases,2020,"RDF triplestores and property graph databases are two approaches for data management which are based on modeling, storing and querying graph-like data. In spite of such common principle, they present special features that complicate the task of database interoperability. While there exist some methods to transform RDF graphs into property graphs, and vice versa, they lack compatibility and a solid formal foundation. This paper presents three direct mappings (schema-dependent and schema-independent) for transforming an RDF database into a property graph database, including data and schema. We show that two of the proposed mappings satisfy the properties of semantics preservation and information preservation. The existence of both mappings allows us to conclude that the property graph data model subsumes the information capacity of the RDF data model.",10.1109/ACCESS.2020.2993117
93e6ba25e4c45c3f204a09b6a6b8b02bd3596091,Querying in the Age of Graph Databases and Knowledge Graphs,2021,"Graphs have become the best way we know of representing knowledge. The computing community has investigated and developed the support for managing graphs by means of digital technology. Graph databases and knowledge graphs surface as the most successful solutions to this program. This tutorial will provide a conceptual map of the data management tasks underlying these developments, paying particular attention to data models and query languages for graphs",10.1145/3448016.3457545
06424dcd3be195738b653d2ee4358d9974f4cb30,GAWD: graph anomaly detection in weighted directed graph databases,2021,"Given a set of node-labeled directed weighted graphs, how to find the most anomalous ones? How can we summarize the normal behavior in the database without losing information? We propose GAWD, for detecting anomalous graphs in directed weighted graph databases. The idea is to (1) iteratively identify the ""best"" substructure (i.e., subgraph or motif) that yields the largest compression when each of its occurrences is replaced by a super-node, and (2) score each graph by how much it compresses over iterations --- the more the compression, the lower the anomaly score. Different from existing work [1] on which we build, GAWD exhibits (i) a lossless graph encoding scheme, (ii) ability to handle numeric edge weights, (iii) interpretability by common patterns, and (iv) scalability with running time linear in input size. Experiments on four datasets injected with anomalies show that GAWD achieves significantly better results than state-of-the-art baselines.",10.1145/3487351.3488325
bcca5c7e9180c4890936343d79e812cb16cbefb6,"Graph Databases Comparison: AllegroGraph, ArangoDB, InfiniteGraph, Neo4J, and OrientDB",2018,"Graph databases are a very powerful solution for storing and searching for data designed for data rich in relationships, such as Facebook and Twitter. With data multiplication and data type diversity there has been a need to create new storage and analysis platforms that structure irregular data with a flexible schema, maintaining a high level of performance and ensuring data scalability effectively, which is a problem that relational databases cannot handle. In this paper, we analyse the most popular graph databases: AllegroGraph, ArangoDB, InfiniteGraph, Neo4J and OrientDB. We study the most important features for a complete and effective application, such as flexible schema, query language, sharding and scalability.",10.5220/0006910203730380
0443c9a0fbb403d5e7d3593f5d286cb74ed0b490,GHashing: Semantic Graph Hashing for Approximate Similarity Search in Graph Databases,2020,"Graph similarity search aims to find the most similar graphs to a query in a graph database in terms of a given proximity measure, say Graph Edit Distance (GED). It is a widely studied yet still challenging problem. Most of the studies are based on the pruning-verification framework, which first prunes non-promising graphs and then conducts verification on the small candidate set. Existing methods are capable of managing databases with thousands or tens of thousands of graphs, but fail to scale to even larger database, due to their exact pruning strategy. Inspired by the recent success of deep-learning-based semantic hashing in image and document retrieval, we propose a novel graph neural network (GNN) based semantic hashing, i.e. GHashing, for approximate pruning. We first train a GNN with ground-truth GED results so that it learns to generate embeddings and hash codes that preserve GED between graphs. Then a hash index is built to enable graph lookup in constant time. To answer a query, we use the hash codes and the continuous embeddings as two-level pruning to retrieve the most promising candidates, which are sent to the exact solver for final verification. Due to the approximate pruning strategy leveraged by our graph hashing technique, our approach achieves significantly faster query time compared to state-of-the-art methods while maintaining a high recall. Experiments show that our approach is on average 20x faster than the only baseline that works on million-scale databases, which demonstrates GHashing successfully provides a new direction in addressing graph search problem for large-scale graph databases.",10.1145/3394486.3403257
c858ba9bce833896b49878a6793dbb382ed83c9e,Query Languages for Graph Databases,2019,"In the past few years, many NoSQL databases have emerged, including graph databases. NoSQL databases have certain advantages and they can be used in certain domains as an alternative to relational databases. In order to use graph databases, one needs to be familiar with specific languages like Cypher Query Language (CQL) or Gremlin. However, some statements in CQL can be considered too complex for end users as it is shown later on. Because of that, the main idea of this chapter is to explore two other languages for graph databases. One of them is new and it is used to pose queries visually. Since CQL does not support recursion, views, etc., the other language is used to show how to use recursion and views on a graph database.",10.4018/978-1-5225-2255-3.CH176
f3e765ca2b8c4191759b141d2e5c42215ed9090d,The Suitability of Graph Databases for Big Data Analysis: A Benchmark,2020,": Digitalization of our society brings various new digital ecosystems (e.g., Smart Cities, Smart Buildings, Smart Mobility), which rely on the collection, storage, and processing of Big Data. One of the recently popular advancements in Big Data storage and processing are the graph databases. A graph database is specialized to handle highly connected data, which can be, for instance, found in the cross-domain setting where various levels of data interconnection take place. Existing works suggest that for data with many relationships, the graph databases perform better than non-graph databases. However, it is not clear where are the borders for speciﬁc query types, for which it is still efﬁcient to use a graph database. In this paper, we design and perform tests that examine these borders. We perform the tests in a cluster of three machines so that we explore the database behavior in Big Data scenarios concerning the query. We speciﬁcally work with Neo4j as a representative of graph databases and PostgreSQL as a representative of non-graph databases.",10.5220/0009350902130220
689c7a86d973895a14feb984b8faa3d5581038ef,"Demystifying Graph Databases: Analysis and Taxonomy of Data Organization, System Designs, and Graph Queries",2019,"Numerous irregular graph datasets, for example social networks or web graphs, may contain even trillions of edges. Often, their structure changes over time and they have domain-specific rich data associated with vertices and edges. Graph database systems such as Neo4j enable storing, processing, and analyzing such large, evolving, and rich datasets. Due to the sheer size and irregularity of such datasets, these systems face unique design challenges. To facilitate the understanding of this emerging domain, we present the first survey and taxonomy of graph database systems. We focus on identifying and analyzing fundamental categories of these systems (e.g., document stores, tuple stores, native graph database systems, or object-oriented systems), the associated graph models (e.g., Resource Description Framework or Labeled Property Graph), data organization techniques (e.g., storing graph data in indexing structures or dividing data into records), and different aspects of data distribution and query execution (e.g., support for sharding and Atomicity, Consistency, Isolation, Durability). Fifty-one graph database systems are presented and compared, including Neo4j, OrientDB, and Virtuoso. We outline graph database queries and relationships with associated domains (NoSQL stores, graph streaming, and dynamic graph algorithms). Finally, we outline future research and engineering challenges related to graph databases.",10.1145/3604932
44ec06d911eed44ee99be61bd9ce964a824a2ed7,Schema Validation and Evolution for Graph Databases,2019,"Despite the maturity of commercial graph databases, little consensus has been reached so far on the standardization of data definition languages (DDLs) for property graphs (PG). The discussion on the characteristics of PG schemas is ongoing in many standardization and community groups. Although some basic aspects of a schema are already present in Neo4j 3.5, like in most commercial graph databases, full support is missing allowing to constraint property graphs with more or less flexibility. In this paper, we focus on two different perspectives from which a PG schema should be considered, as being descriptive or prescriptive, and we show how it would be possible to switch from one to another as the application under development gains more stability. Apart from proposing concise schema DDL inspired by Cypher syntax, we show how schema validation can be enforced through homomorphisms between PG schemas and PG instances; and how schema evolution can be described through the use of graph rewriting operations. Our prototypical implementation demonstrates feasibility and shows the need of offering high-level query primitives to accommodate flexible graph schema requirements as showcased in our work.",10.1007/978-3-030-33223-5_37
8afa626901e6b77b3936b8eb64df4a5a538444a0,Using Graph Databases,2020,"Data in High Energy Physics (HEP) usually consist of complext complex data structures stored in relational databases and files with internal schema. Such architecture exhibits many shortcomings, which could be fixed by migrating into Graph Database storage. The paper describes basic principles of the Graph Database together with an overview of existing standards and implementations. The usefulness and usability are demonstrated using the concrete example of the Event Index of the ATLAS experiment at LHC in two approaches as the full storage (all data are in the Graph Database) and meta-storage (a layer of schema-less graph-like data implemented on top of more traditional storage). The usability, the interfaces with the surrounding framework and the performance of those solutions are discussed. The possible more general usefulness for generic experiments’ storage is also discussed.",10.1051/epjconf/202024504004
6ea38aacc779506909d425aa19114b92bb1854f8,Regular Queries on Graph Databases,2017,"Graph databases are currently one of the most popular paradigms for storing data. One of the key conceptual differences between graph and relational databases is the focus on navigational queries that ask whether some nodes are connected by paths satisfying certain restrictions. This focus has driven the definition of several different query languages and the subsequent study of their fundamental properties. We define the graph query language of Regular Queries, which is a natural extension of unions of conjunctive 2-way regular path queries (UC2RPQs) and unions of conjunctive nested 2-way regular path queries (UCN2RPQs). Regular queries allow expressing complex regular patterns between nodes. We formalize regular queries as nonrecursive Datalog programs extended with the transitive closure of binary predicates. This language has been previously considered, but its algorithmic properties are not well understood. Our main contribution is to show elementary tight bounds for the containment problem for regular queries. Specifically, we show that this problem is 2Expspace-complete. For all extensions of regular queries known to date, the containment problem turns out to be non-elementary. Together with the fact that evaluating regular queries is not harder than evaluating UCN2RPQs, our results show that regular queries achieve a good balance between expressiveness and complexity, and constitute a well-behaved class that deserves further investigation.",10.1007/s00224-016-9676-2
59f1df2f60280eea79d195b9c561d52a77fb24de,Foundations of Modern Query Languages for Graph Databases,2016,"We survey foundational features underlying modern graph query languages. We first discuss two popular graph data models: edge-labelled graphs, where nodes are connected by directed, labelled edges, and property graphs, where nodes and edges can further have attributes. Next we discuss the two most fundamental graph querying functionalities: graph patterns and navigational expressions. We start with graph patterns, in which a graph-structured query is matched against the data. Thereafter, we discuss navigational expressions, in which patterns can be matched recursively against the graph to navigate paths of arbitrary length; we give an overview of what kinds of expressions have been proposed and how they can be combined with graph patterns. We also discuss several semantics under which queries using the previous features can be evaluated, what effects the selection of features and semantics has on complexity, and offer examples of such features in three modern languages that are used to query graphs: SPARQL, Cypher, and Gremlin. We conclude by discussing the importance of formalisation for graph query languages; a summary of what is known about SPARQL, Cypher, and Gremlin in terms of expressivity and complexity; and an outline of possible future directions for the area.",10.1145/3104031
1d0d84a1f4e8de75151d88bd152816f18109d032,Similarity Search in Graph Databases: A Multi-Layered Indexing Approach,2017,"We consider in this paper the similarity search problem that retrieves relevant graphs from a graph database under the well-known graph edit distance (GED) constraint. Formally, given a graph database G = fg1, g2, …, gng and a query graph q, we aim to search the graph gi 2 G such that the graph edit distance between gi and q, GED(gi, q), is within a userspecified GED threshold. In spite of its theoretical significance and wide applicability, the GED-based similarity search problem is challenging in large graph databases due in particular to a large amount of GED computation incurred, which has proven to be NP-hard. In this paper, we propose a parameterized, partitionbased GED lower bound that can be instantiated into a series of tight lower bounds towards synergistically pruning false-positive graphs from G before costly GED computation is performed. We design an efficient, selectivity-aware algorithm to partition graphs of G into highly selective subgraphs. They are further incorporated in a cost-effective, multi-layered indexing structure, MLIndex (Multi-Layered Index), for GED lower bound crosschecking and false-positive graph filtering with theoretical performance guarantees. Experimental studies in real and synthetic graph databases validate the efficiency and effectiveness of ML-Index, which achieves up to an order of magnitude speedup over the state-of-the-art method for similarity search in graph databases.",10.1109/ICDE.2017.129
6e87f544017f73670c44d9d0856340a289ff9e38,"SQUID: A Scalable System for Querying, Updating and Indexing Dynamic Graph Databases",2019,"Graph databases such as chemical databases, protein databases, and RNA motif databases, are simply a collection of graphs. Querying a graph database involves the computation of a subgraph isomorphism problem (which is NP-complete) for each graph in the database. Therefore, an index is required to filter out false positives and reduce the number of subgraph isomorphisms to compute. In this demo, we introduce SQUID, a scalable system for querying, updating and indexing dynamic graph databases, i.e., databases changing over time, and showcase it on chemical databases. The tool uses a graph coarsening-based index that is able to answer both subgraph and supergraph queries. It also allows the database to be changed with an automatic index update. Also, it displays information found in the graph database in a concise manner that is easier to understand.",10.1145/3335783.3335799
659d8b24e53e8adfbcb1f16b26ada2cb8afa2c85,A Schema-First Formalism for Labeled Property Graph Databases: Enabling Structured Data Loading and Analytics,2019,"Graph databases provide better support for highly interconnected datasets than relational databases. However, labeled property graph databases, which have become increasingly popular, are schema-optional, making them prone to data corruption, especially when new users switch from relational databases to graph databases. In this work, we provide a schema-driven formalism for graph databases. This formalism enables schema-driven loading of graph databases from other sources, such as relational databases. Also, this formalism enables schema-driven data analytics that allows for a more structured analysis of data stored in graph databases. Such analytics are based on a boilerplate approach allowing users who are not experts in the use of graph database query languages to carry out analytics efficiently. We showcase the utility of the proposed formalism by considering a case study from Airbnb for illustrating schema-based loading procedures. The proposed schema-driven analytics process is illustrated using another case study from an industrial cyber-physical systems standard. Overall, the schema-driven formalism provides several useful features, such as preventing both data corruption and long-term degradation of graph database structures.",10.1145/3365109.3368782
b8e30590ac81875a7fc4f7cfd425bd5148a72f5c,Updating Graph Databases with Cypher,2019,"The paper describes the present and the future of graph updates in Cypher, the language of the Neo4j property graph database and several other products. Update features include those with clear analogs in relational databases, as well as those that do not correspond to any relational op-erators. Moreover, unlike SQL, Cypher updates can be ar-bitrarily intertwined with querying clauses. After present-ing the current state of update features, we point out their shortcomings, most notably violations of atomicity and nondeterministic behavior of updates. These have not been previously known in the Cypher community. We then describe the industry-academia collaboration on designing a revised set of Cypher update operations. Based on discovered shortcomings of update features, a number of possible solutions were devised. They were presented to key Cypher users, who were given the opportunity to comment on how update features are used in real life, and on their preferences for proposed ﬁxes. As the result of the consultation, a new set of update operations for Cypher were designed. Those led to a streamlined syntax, and eliminated the unexpected and problematic behavior that original Cypher updates exhib-ited.",10.14778/3352063.3352139
c867c372ed78e27c53feaf788e74aa7506302bbe,Graph Databases: Neo4j Analysis,2017,"The volume of data is growing at an increasing rate. This growth is both in size and in connectivity, where connectivity refers to the increasing presence of relationships between data. Social networks such as Facebook and Twitter store and process petabytes of data each day. Graph databases have gained renewed interest in the last years, due to their applications in areas such as the Semantic Web and Social Network Analysis. Graph databases provide an effective and efficient solution to data storage and querying data in these scenarios, where data is rich in relationships. In this paper, it is analyzed the fundamental points of graph databases, showing their main characteristics and advantages. We study Neo4j, the top graph database software in the market and evaluate its performance using the Social Network Benchmark (SNB).",10.5220/0006356003510356
d154d986c3a402c207c6c0e22093f2e1a878fe71,Two for one: querying property graph databases using SPARQL via gremlinator,2018,"In the past decade Knowledge graphs have become very popular and frequently rely on the Resource Description Framework (RDF) or Property Graphs (PG) as their data models. However, the query languages for these two data models - SPARQL for RDF and the PG traversal language Gremlin - are lacking basic interoperability. In this demonstration paper, we present Gremlinator, the first translator from SPARQL - the W3C standardized language for RDF - to Gremlin - a popular property graph traversal language. Gremlinator translates SPARQL queries to Gremlin path traversals for executing graph pattern matching queries over graph databases. This allows a user, who is well versed in SPARQL, to access and query a wide variety of Graph databases avoiding the steep learning curve for adapting to a new Graph Query Language (GQL). Gremlin is a graph computing system-agnostic traversal language (covering both OLTP graph databases and OLAP graph processors), making it a desirable choice for supporting interoperability for querying Graph databases. Gremlinator is planned to be released as an Apache TinkerPop plugin in the upcoming releases.",10.1145/3210259.3210271
2be77516f6233a9d3ebbf93b439c3658d955d2e5,Efficient and Scalable Integrity Verification of Data and Query Results for Graph Databases,2018,"Graphs are used for representing and understanding objects and their relationships for numerous applications such as social networks, Semantic Webs, and biological networks. Integrity assurance of data and query results for graph databases is an essential security requirement. In this paper, we propose two efficient integrity verification schemes—HMACs for graphs (gHMAC) for two-party data sharing, and redactable HMACs for graphs (rgHMAC) for third-party data sharing, such as a cloud-based graph database service. We compute one HMAC value for both the schemes and two other verification objects for rgHMAC scheme that are shared with the verifier. We show that the proposed schemes are provably secure with respect to integrity attacks on the structure and/or content of graphs and query results. The proposed schemes have linear complexity in terms of the number of vertices and edges in the graphs, which is shown to be optimal. Our experimental results corroborate that the proposed HMAC-based schemes for graphs are highly efficient as compared to the digital signature-based schemes—computation of HMAC tags is about 10 times faster than the computation of digital signatures.",10.1109/TKDE.2017.2776221
3876267a762615ac876df561b900aa6e564fc9b4,Performance of Graph and Relational Databases in Complex Queries,2022,"In developing NoSQL databases, a major motivation is to achieve better efficient query performance compared with relational databases. The graph database is a NoSQL paradigm where navigation is based on links instead of joining tables. Links can be implemented as pointers, and following a pointer is a constant time operation, whereas joining tables is more complicated and slower, even in the presence of foreign keys. Therefore, link-based navigation has been seen as a more efficient query approach than using join operations on tables. Existing studies strongly support this assumption. However, query complexity has received less attention. For example, in enterprise information systems, queries are usually complex so data need to be collected from several tables or by traversing paths of graph nodes of different types. In the present study, we compared the query performance of a graph-based database system (Neo4j) and relational database systems (MySQL and MariaDB). The effect of different efficiency issues (e.g., indexing and optimization) were included in the comparison in order to investigate the most efficient solutions for different query types. The outcome is that although Neo4j is more efficient for simple queries, MariaDB is essentially more efficient when the complexity of queries increases. The study also highlighted how dramatically the efficiency of relational database has grown during the last decade.",10.3390/app12136490
1f4bee77bb69b92dd16ce0c3ad0f477f4ecf1d30,Analytical queries on semantic trajectories using graph databases,2019,"This article studies the analysis of moving object data collected by location‐aware devices, such as GPS, using graph databases. Such raw trajectories can be transformed into so‐called semantic trajectories, which are sequences of stops that occur at “places of interest.” Trajectory data analysis can be enriched if spatial and non‐spatial contextual data associated with the moving objects are taken into account, and aggregation of trajectory data can reveal hidden patterns within such data. When trajectory data are stored in relational databases, there is an “impedance mismatch” between the representation and storage models. Graphs in which the nodes and edges are annotated with properties are gaining increasing interest to model a variety of networks. Therefore, this article proposes the use of graph databases (Neo4j in this case) to represent and store trajectory data, which can thus be analyzed at different aggregation levels using graph query languages (Cypher, for Neo4j). Through a real‐world public data case study, the article shows that trajectory queries are expressed more naturally on the graph‐based representation than over the relational alternative, and perform better in many typical cases.",10.1111/tgis.12556
39af4070dca3221d275898c55884167bc03fe73d,Singleton Property Graph: Adding A Semantic Web Abstraction Layer to Graph Databases,2019,,
6e4e15ab0d55232b93e76ddecb6c969a0d18f853,Designing Graph Databases With GRAPHED,2019,"In recent years, graph database systems have become very popular and been deployed mainly in situations where the relationship between data is significant, such as in social networks. Although they do not require a particular schema design, a data model contributes to their consistency. Designing diagrams is an approach to satisfying this demand for a conceptual data model. While researchers and companies have been developing concepts and notations for graph database modeling, their notations focus on their specific implementations. In this article, the authors propose a diagram to address this lack of a generic and comprehensive notation for graph databases modeling, named GRAPHED (Graph Description Diagram for Graph Databases). The authors verified the effectiveness and compatibility of GRAPHED in two case studies: fraud identification, and a biological network model.",10.4018/JDM.2019010103
647d0e540627c5a903299a90d20530f8e48c18d9,Representing and querying disease networks using graph databases,2016,"Systems biology experiments generate large volumes of data of multiple modalities and this information presents a challenge for integration due to a mix of complexity together with rich semantics. Here, we describe how graph databases provide a powerful framework for storage, querying and envisioning of biological data. We show how graph databases are well suited for the representation of biological information, which is typically highly connected, semi-structured and unpredictable. We outline an application case that uses the Neo4j graph database for building and querying a prototype network to provide biological context to asthma related genes. Our study suggests that graph databases provide a flexible solution for the integration of multiple types of biological data and facilitate exploratory data mining to support hypothesis generation.",10.1186/s13040-016-0102-8
13bd0e08713f0b340396634892d2c9e3c5bee548,Comparative study of relational and graph databases,2019,"We live in a time of incredible technological development. In a fraction of a second, millions of information are generated and processed all over the world. The need to develop new models of databases is related to the dynamic and the amount of data stored. Using database schemes which are rigid and do not quickly adapt to changes occurring in the real world is ineffective and sometimes simply impossible. The solution of all these problems could be, among others, graph databases.In this talk, we present a detailed comparison of relational and graph databases, compiling the stages of design, implementation and testing. At each stage, we keep the same data in both databases, checking how easily they can be modelled in each logical model, then imported, and finally made queries and modifications on them.",10.1109/Informatics47936.2019.9119303
e8505f6175585713aa7b785285b3abc4113b57ef,Investigative Graph Search using Graph Databases,2019,"Identification and tracking of individuals or groups perpetrating latent or emergent behaviors are significant in home-land security, cyber security, behavioral health, and consumer analytics. Graphs provide an effective formal mechanism to capture the relationships among individuals of interest as well as their behavior patterns. Graph databases, developed recently, serve as convenient data stores for such complex graphs and allow efficient retrievals via high-level libraries and the ability to implement custom queries. We introduce PINGS (Procedures for Investigative Graph Search) a graph database library of procedures for investigative search. We develop an inexact graph pattern matching technique and scoring mechanism within the database as custom procedures to identify latent behavioral patterns of individuals. It addresses, among other things, sub-graph isomorphism, an NP-hard problem, via an investigative search in graph databases. We demonstrate the capability of detecting such individuals and groups meeting query criteria using two data sets, a synthetically generated radicalization dataset and a publicly available crime dataset.",10.1109/GC46384.2019.00017
1731458f7ac96b23bd8ffe4c1bb71295772b5566,Modelling and Querying Star and Snowflake Warehouses Using Graph Databases,2019,,10.1007/978-3-030-30278-8_18
8ada773522e0cc1b1a1b36b8e5d85417794a875f,"When Subgraph Isomorphism is Really Hard, and Why This Matters for Graph Databases",2018,"The subgraph isomorphism problem involves deciding whether a copy of a pattern graph occurs inside a larger target graph. The non-induced version allows extra edges in the target, whilst the induced version does not. Although both variants are NP-complete, algorithms inspired by constraint programming can operate comfortably on many real-world problem instances with thousands of vertices. However, they cannot handle arbitrary instances of this size. We show how to generate "" really hard "" random instances for subgraph isomorphism problems, which are computationally challenging with a couple of hundred vertices in the target, and only twenty pattern vertices. For the non-induced version of the problem, these instances lie on a satisfiable / unsatisfiable phase transition, whose location we can predict; for the induced variant, much richer behaviour is observed, and constrained-ness gives a better measure of difficulty than does proximity to a phase transition. These results have practical consequences: we explain why the widely researched "" filter / verify "" indexing technique used in graph databases is founded upon a misunderstanding of the empirical hardness of NP-complete problems, and cannot be beneficial when paired with any reasonable subgraph isomorphism algorithm.",10.1613/jair.5768
4f873d5a3ab80402e206a361eeffd3b69f014db0,Semiring Provenance over Graph Databases,2018,,
b3b64555a641afd3c8fc2da30bbb30b0b56e0c60,Comparative Analysis of Relational and Graph Databases for Social Networks,2018,"Network analysis is an important concept of the 21st- century business analytics. Software applications provide diverse capabilities to scale data and information in making constructive decisions. Business intelligence is the process through which data sets are arranged in graphs and tables to derive patterns that can help predict consumption, demand and future trends. The Neo4j is a database purpose-built to support data analysis, graph storage, and intelligent applications. The database supports direct access, data management, and graph visualization for social channels and digital marketing in social platforms. This paper is a comparative analysis that examines the operational efficiency of Neo4j and another selected database and to examine their overall performance and functionality. Of special concern for research is to propose an experiment to evaluate the performance, scope of operations, and overall functionality of the two graph databases. The objective of the comparative analysis is to determine the most appropriate database for social networks. The results of the experience will help inform business leaders and I. T managers on the best applications and databases to deploy in the management of social networking channels like Facebook and Twitter. (Abstract)",10.1109/CAIS.2018.8441982
12b8af038d27defddc972c9bbcdf151b2594ee69,A Bottom-Up Algorithm for Answering Context-Free Path Queries in Graph Databases,2018,,10.1007/978-3-319-91662-0_17
d47686fd5ce403fe962c80b651c8b130061a5d73,UMLtoGraphDB: Mapping Conceptual Schemas to Graph Databases,2016,"The need to store and manipulate large volume of (unstructured) data has led to the development of several NoSQL databases for better scalability. Graph databases are a particular kind of NoSQL databases that have proven their efficiency to store and query highly interconnected data, and have become a promising solution for multiple applications. While the mapping of conceptual schemas to relational databases is a well-studied field of research, there are only few solutions that target conceptual modeling for NoSQL databases and even less focusing on graph databases. This is specially true when dealing with the mapping of business rules and constraints in the conceptual schema. In this article we describe a mapping from UML/OCL conceptual schemas to Blueprints, an abstraction layer on top of a variety of graph databases, and Gremlin, a graph traversal language, via an intermediate Graph metamodel. Tool support is fully available.",10.1007/978-3-319-46397-1_33
e0fdd11f8ffe065081e2e4694a8855a766abe31a,Improving Game Modeling for the Quoridor Game State Using Graph Databases,2018,,10.1007/978-3-319-73450-7_32
620f78ee4a52c3f61e5600d5c354b2234d8f7858,Fireplug: Flexible and robust N-version geo-replication of graph databases,2018,"The paper describes and evaluates Fireplug, a flexible architecture to build robust geo-replicated graph databases. Fireplug can be configured to tolerate from crash to Byzantine faults, both within and across different datacenters. Furthermore, Fireplug is robust to bugs in existing graph database implementations, as it allows to combine multiple graph databases instances in a cohesive manner. Thus, Fireplug can support many different deployments, according to the performance/robustness tradeoffs imposed by the target application. Our evaluation shows that Fireplug can implement Byzantine fault tolerance in geo-replicated scenarios and still outperform the built-in replication mechanism of Neo4j, which only supports crash faults.",10.1109/ICOIN.2018.8343095
006bd41c4a52c64445afdbc67182715a476e09c8,Querying Encrypted Graph Databases,2018,"Copyright © 2018 by SCITEPRESS – Science and Technology Publications, Lda. All rights reserved. We present an approach to execution of queries on encrypted graph databases. The approach is inspired by CryptDB system for relational DBs (R. A. Popa et al). Before processing a graph query is translated into encrypted form which then executed on a server without decrypting any data; the encrypted results are sent back to a client where they are finally decrypted. In this way data privacy is protected at the server side. We present the design of the system and empirical data obtained by experimentation with a prototype, implemented for Neo4j graph DBMS and Cypher query language, utilizing Java API. We report the efficiency of query execution for various types of queries on encrypted and non-encrypted Neo4j graph databases.",10.5220/0006660004470451
0134db5adf50c471906cbffa0d2f6a4fcbc0ae01,Graph Databases in Molecular Biology,2018,,10.1007/978-3-030-01722-4_5
f0dde44d5836ac8ae1d8a501b26317c952fe5abc,Using Functional Dependencies in Conversion of Relational Databases to Graph Databases,2018,,10.1007/978-3-319-98812-2_31
d7948ee13698eaa556716b4161202f8690e493ff,Application of graph databases and graph theory concepts for advanced analysing of BIM models based on IFC standard,2017,,
16a65dcd609d48f79a9a271377628fdae5315a62,biochem4j: Integrated and extensible biochemical knowledge through graph databases,2017,"Biologists and biochemists have at their disposal a number of excellent, publicly available data resources such as UniProt, KEGG, and NCBI Taxonomy, which catalogue biological entities. Despite the usefulness of these resources, they remain fundamentally unconnected. While links may appear between entries across these databases, users are typically only able to follow such links by manual browsing or through specialised workflows. Although many of the resources provide web-service interfaces for computational access, performing federated queries across databases remains a non-trivial but essential activity in interdisciplinary systems and synthetic biology programmes. What is needed are integrated repositories to catalogue both biological entities and–crucially–the relationships between them. Such a resource should be extensible, such that newly discovered relationships–for example, those between novel, synthetic enzymes and non-natural products–can be added over time. With the introduction of graph databases, the barrier to the rapid generation, extension and querying of such a resource has been lowered considerably. With a particular focus on metabolic engineering as an illustrative application domain, biochem4j, freely available at http://biochem4j.org, is introduced to provide an integrated, queryable database that warehouses chemical, reaction, enzyme and taxonomic data from a range of reliable resources. The biochem4j framework establishes a starting point for the flexible integration and exploitation of an ever-wider range of biological data sources, from public databases to laboratory-specific experimental datasets, for the benefit of systems biologists, biosystems engineers and the wider community of molecular biologists and biological chemists.",10.1371/journal.pone.0179130
f75f16b5cf0fc2e667d79a122190757bfccb0a3d,GRAPHED: A Graph Description Diagram for Graph Databases,2018,,10.1007/978-3-319-77703-0_111
b374939320dcba4255a14637d36bbee785a2d860,Building Knowledge Extraction from BIM/IFC Data for Analysis in Graph Databases,2018,,10.1007/978-3-319-91262-2_57
d427db1ba2cc92b36d16a5e5114f1027c90fff68,Model driven reverse engineering of NoSQL property graph databases: The case of Neo4j,2017,"Most NoSQL databases are schemaless. Although they offer some flexibility, they do not have any knowledge of the database schema, losing the benefits provided by these schemas. It is generally accepted that data modelling can have an impact on performance, consistency, usability, and maintainability. We argue that NoSQL databases need data models that ensure the proper storage and the relevant querying of the data. This paper seeks to present and illustrate an MDA-based approach, allowing us to achieve a reverse engineering of NoSQL property graph databases into an Extended Entity-Relationship schema. The approach is applied to the case of Neo4j graph database. We present an illustrative scenario and evaluate the reverse engineering approach.",10.1109/BIGDATA.2017.8257957
7549a6e03afdd42ae94c19235db076319eeb34f8,Do We Need Specialized Graph Databases?: Benchmarking Real-Time Social Networking Applications,2017,"With the advent of online social networks, there is an increasing demand for storage and processing of graph-structured data. Social networking applications pose new challenges to data management systems due to demand for real-time querying and manipulation of the graph structure. Recently, several systems specialized systems for graph-structured data have been introduced. However, whether we should abandon mature RDBMS technology for graph databases remains an ongoing discussion. In this paper we present an graph database benchmarking architecture built on the existing LDBC Social Network Benchmark. Our proposed architecture stresses the systems with an interactive transactional workload to better simulate the real-time nature of social networking applications. Using this improved architecture, we evaluated a selection of specialized graph databases, RDF stores, and RDBMSes adapted for graphs. We do not find that specialized graph databases provide definitively better performance.",10.1145/3078447.3078459
ae876b07435880d0359ca68b30af90b2e3193ad4,IOGP: An Incremental Online Graph Partitioning Algorithm for Distributed Graph Databases,2017,"Graphs have become increasingly important in many applications and domains such as querying relationships in social networks or managing rich metadata generated in scientific computing. Many of these use cases require high-performance distributed graph databases for serving continuous updates from clients and, at the same time, answering complex queries regarding the current graph. These operations in graph databases, also referred to as online transaction processing (OLTP) operations, have specific design and implementation requirements for graph partitioning algorithms. In this research, we argue it is necessary to consider the connectivity and the vertex degree changes during graph partitioning. Based on this idea, we designed an Incremental Online Graph Partitioning (IOGP) algorithm that responds accordingly to the incremental changes of vertex degree. IOGP helps achieve better locality, generate balanced partitions, and increase the parallelism for accessing high-degree vertices of the graph. Over both real-world and synthetic graphs, IOGP demonstrates as much as 2x better query performance with a less than 10% overhead when compared against state-of-the-art graph partitioning algorithms.",10.1145/3078597.3078606
45063cf2e0116e700da5ca2863c8bb82ad4d64c2,Conceptual and Database Modelling of Graph Databases,2016,"Comparing graph databases with traditional, e.g., relational databases, some important database features are often missing there. Particularly, a graph database schema including integrity constraints is not explicitly defined, also a conceptual modelling is not used at all. It is hard to check a consistency of the graph database, because almost no integrity constraints are defined. In the paper, we discuss these issues and present current possibilities and challenges in graph database modelling. Also a conceptual level of a graph database design is considered. We propose a sufficient conceptual model and show its relationship to a graph database model. We focus also on integrity constraints modelling functional dependencies between entity types, which reminds modelling functional dependencies known from relational databases and extend them to conditional functional dependencies.",10.1145/2938503.2938547
e89c22555d5e20a14669522838029c6f49893c57,"Querying Wikidata: Comparing SPARQL, Relational and Graph Databases",2016,,10.1007/978-3-319-46547-0_10
6cc4b0919f14bdcba4d3f1d97b04bd134bfb45c4,STON: exploring biological pathways using the SBGN standard and graph databases,2016,"When modeling in Systems Biology and Systems Medicine, the data is often extensive, complex and heterogeneous. Graphs are a natural way of representing biological networks. Graph databases enable efficient storage and processing of the encoded biological relationships. They furthermore support queries on the structure of biological networks. We present the Java-based framework STON (SBGN TO Neo4j). STON imports and translates metabolic, signalling and gene regulatory pathways represented in the Systems Biology Graphical Notation into a graph-oriented format compatible with the Neo4j graph database. STON exploits the power of graph databases to store and query complex biological pathways. This advances the possibility of: i) identifying subnetworks in a given pathway; ii) linking networks across different levels of granularity to address difficulties related to incomplete knowledge representation at single level; and iii) identifying common patterns between pathways in the database.",10.1186/s12859-016-1394-x
f51dfc161c98140d4156e512dd3b4f839cc7f491,Scaling Lifted Probabilistic Inference and Learning Via Graph Databases,2016,"Over the past decade, exploiting relations and symmetries within probabilistic models has been proven to be surprisingly effective at solving large scale data mining problems. One of the key operations inside these lifted approaches is counting be it for parameter/structure learning or for efficient inference. Typically, however, they just count exploiting the logical structure using adhoc operators. This paper investigates whether ‘Compilation to Graph Databases’ could be a practical technique for scaling lifted probabilistic inference and learning methods. We demonstrate that the proposed approach achieves reasonable speed-ups for both inference and learning, without sacrificing performance.",10.1137/1.9781611974348.83
f7a8573d4daa4083419a864ef7a8bf97bc0754e5,Detecting Evidence of Fraud in the Brazilian Government Using Graph Databases,2017,,10.1007/978-3-319-56538-5_47
e7f1640a76bbfdf701e6cd4d81163be2eb97a4de,Functional querying in graph databases,2017,"The paper is focused on a functional querying in graph databases. We consider labelled property graph model and mention also the graph model behind XML databases. An attention is devoted to functional modelling of graph databases both at a conceptual and data level. The notions of graph conceptual schema and graph database schema are considered. The notion of a typed attribute is used as a basic structure both on the conceptual and database level. As a formal approach to declarative graph database querying a version of typed lambda calculus is used. This approach allows to use a logic necessary for querying, arithmetic as well as aggregation function. Another advantage is the ability to deal with relations and graphs in one integrated environment.",10.1007/s40595-017-0104-6
f3b0ca9d0c420e6c6bc24a5e9dd959e8c899b38e,Backlogs and Interval Timestamps: Building Blocks for Supporting Temporal Queries in Graph Databases,2017,,
42fe18be6e9ff2c11e5b2347c93e131c9abc7a51,Graph Databases for Knowledge Management,2017,"Emerging technologies let companies manage their knowledge assets with more innovative and effective methods. Due to the complex nature of knowledge management processes, it is cumbersome to design, develop, and implement a system based on relational databases. This article proposes a specific graph database application in streamlining major knowledge management processes. The author develops a property graph data model to facilitate the process model of knowledge management. In addition, this property graph data model is implemented through the Neo4j graph database system. This research provides some guidance for practitioners in seeking alternative approaches to traditional methods of knowledge management.",10.1109/MITP.2017.4241463
91bd879d3fcbdb851c8442be47e7ee8fa4ef8586,Scalable supergraph search in large graph databases,2016,"Supergraph search is a fundamental problem in graph databases that is widely applied in many application scenarios. Given a graph database and a query-graph, supergraph search retrieves all data-graphs contained in the query-graph from the graph database. Most existing solutions for supergraph search follow the pruning-and-verification framework, which prunes false answers based on features in the pruning phase and performs subgraph isomorphism testings on the remaining graphs in the verification phase. However, they are not scalable to handle large-sized data-graphs and query-graphs due to three drawbacks. First, they rely on a frequent subgraph mining algorithm to select features which is expensive and cannot generate large features. Second, they require a costly verification phase. Third, they process features in a fixed order without considering their relationship to the query-graph. In this paper, we address the three drawbacks and propose new indexing and query processing algorithms. In indexing, we select features directly from the data-graphs without expensive frequent subgraph mining. The features form a feature-tree that contains all-sized features and both the cost sharing and pruning power of the features are considered. In query processing, we propose a verification-free algorithm, where the order to process features is query-dependent by considering both the cost sharing and the pruning power. We explore two optimization strategies to further improve the algorithm efficiency. The first strategy applies a lightweight graph compression technique and the second strategy optimizes the inclusion of answers. Finally, we conduct extensive performance studies on two real large datasets to demonstrate the high scalability of our algorithms.",10.1109/ICDE.2016.7498237
19c284f50869970a31f45cbec20d2db90aa278e3,Learning to Speed Up Query Planning in Graph Databases,2017,"    Querying graph structured data is a fundamental operation that enables important applications including knowledge graph search, social network analysis, and cyber-network security. However, the growing size of real-world data graphs poses severe challenges for graph databases to meet the response-time requirements of the applications. Planning the computational steps of query processing — Query Planning — is central to address these challenges. In this paper, we study the problem of learning to speedup query planning in graph databases towards the goal of improving the computational-efficiency of query processing via training queries. We present a Learning to Plan (L2P) framework that is applicable to a large class of query reasoners that follow the Threshold Algorithm (TA) approach. First, we define a generic search space over candidate query plans, and identify target search trajectories (query plans) corresponding to the training queries by performing an expensive search. Subsequently, we learn greedy search control knowledge to imitate the search behavior of the target query plans. We provide a concrete instantiation of our L2P framework for STAR, a state-of-the-art graph query reasoner. Our experiments on benchmark knowledge graphs including dbpedia, yago, and freebase show that using the query plans generated by the learned search control knowledge, we can significantly improve the speed of STAR with negligible loss in accuracy.   ",10.1609/icaps.v27i1.13849
6b98410fcea56a1d8368fe31384d2cacdcf78fdc,Striving for semantic convergence with fuzzy cognitive maps and graph databases,2017,"The exponentially rising amounts of urban data demand new conceptual and technical methods for their management and storage. The era of the Semantic Web requires a convergence toward commonly shared meanings. The combination of fuzzy cognitive maps with graph databases is a first approach to a solution. This article determines the basic requirements for the storing of fuzzy cognitive maps to test current graph databases for their structural suitability. Six out of 47 graph databases fulfill the requirements and are thus recommended for further research purposes. As a proof of concept, OrientDB is used to present how a semantic convergence can be reached through the combination of fuzzy cognitive maps and graph databases in a cognitive city by tackling fuzziness.",10.1109/FUZZ-IEEE.2017.8015657
cdadc2aa758d3abc1f77add07232f21df9287162,A Scalable Graph-Coarsening Based Index for Dynamic Graph Databases,2017,"A graph database D is a collection of graphs. To speed up subgraph query answering on graph databases, indexes are commonly used. State-of-the-art graph database indexes do not adapt or scale well to dynamic graph database use; they are static, and their ability to prune possible search responses to meet user needs worsens over time as databases change and grow. Users can re-mine indexes to gain some improvement, but it is time consuming. Users must also tune numerous parameters on an ongoing basis to optimize performance and can inadvertently worsen the query response time if they do not choose parameters wisely. Recently, a one-pass algorithm has been developed to enhance the performance of frequent subgraphs based indexes by using the algorithm to update them regularly. However, there are some drawbacks, most notably the need to make updates as the query workload changes. In this paper, we propose a new index based on graph-coarsening to speed up subgraph query answering time in dynamic graph databases. Our index is parameter-free, query-independent, scalable,small enough to store in the main memory, and is simpler and less costly to maintain for database updates. Experimental results show that our index outperforms hybrid-indexes (i.e. indexes updated with one-pass) for query answering time in the case of social network databases, and is comparable with these indexes for frequent and infrequent queries on chemical databases. Our index can be updated up to 60 times faster in comparison to one-pass on dynamic graph databases. Moreover, our index is independent of the query workload for index update and is up to 15 times faster after hybrid-indexes are attuned to query workload.",10.1145/3132847.3133003
f6dc7097c06f58e93209f5cf14913c63dcfe183f,TASWEET : optimizing disjunctive regular path queries in graph databases,2017,,
a31b391917fbfc24675fc13a6724fe61460b425a,AutoG: a visual query autocompletion framework for graph databases,2016,"Composing queries is evidently a tedious task. This is particularly true of graph queries as they are typically complex and prone to errors, compounded by the fact that graph schemas can be missing or too loose to be helpful for query formulation. Despite the great success of query formulation aids, in particular, automatic query completion, graph query autocompletion has received much less research attention. In this paper, we propose a novel framework for subgraph query autocompletion (called AutoG). Given an initial query q and a user’s preference as input, AutoG returns ranked query suggestions Q′\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Q'$$\end{document} as output. Users may choose a query from Q′\documentclass[12pt]{minimal} \usepackage{amsmath} \usepackage{wasysym} \usepackage{amsfonts} \usepackage{amssymb} \usepackage{amsbsy} \usepackage{mathrsfs} \usepackage{upgreek} \setlength{\oddsidemargin}{-69pt} \begin{document}$$Q'$$\end{document} and iteratively apply AutoG to compose their queries. The novelties of AutoG are as follows: First, we formalize query composition. Second, we propose to increment a query with the logical units called c-prime features that are (i) frequent subgraphs and (ii) constructed from smaller c-prime features in no more than c ways. Third, we propose algorithms to rank candidate suggestions. Fourth, we propose a novel index called feature Dag (FDag) to optimize the ranking. We study the query suggestion quality with simulations and real users and conduct an extensive performance evaluation. The results show that the query suggestions are useful (saved roughly 40% of users’ mouse clicks), and AutoG returns suggestions shortly under a large variety of parameter settings.",10.1007/s00778-017-0454-9
5685a394b25fcb27b6ad91f7325f2e60a9892e2a,Query Optimization Techniques In Graph Databases,2016,"Graph databases (GDB) have recently been arisen to overcome the limits of traditional databases for storing and managing data with graph-like structure. Today, they represent a requirement for many applications that manage graph-like data, like social networks. Most of the techniques, applied to optimize queries in graph databases, have been used in traditional databases, distribution systems... or they are inspired from graph theory. However, their reuse in graph databases should take care of the main characteristics of graph databases, such as dynamic structure, highly interconnected data, and ability to efficiently access data relationships. In this paper, we survey the query optimization techniques in graph databases. In particular, we focus on the features they have introduced to improve querying graph-like data.",10.5121/ijdms.2016.8401
9b3f2b73263ce06a6d61ef2ad9df7cce33e171f7,Application of graph databases for transport purposes,2016,*e-mail: a.czerepicki@wt.pw.edu.pl Abstract. The article presents an innovative concept of applying graph databases in transport information systems. The model of a graph database has been presented together with implementation of data structures and search operations in a graph. The transformation concept of relational model to a graph data model has been developed. The schema of graph database has been proposed for public transport information system purposes. The realization methods have been illustrated by the use of search function based on the Cypher query language.,10.1515/BPASTS-2016-0051
416292827584552604807411db7262d7c28257e6,Plan Before You Execute: A Cost-Based Query Optimizer for Attributed Graph Databases,2016,,10.1007/978-3-319-43946-4_21
325bbe2c92072b16645a5cded3b9eda3cb97660b,Predictive Performance Comparison Analysis of Relational & NoSQL Graph Databases,2017,"From last three decades, the relational databases are being used in many organizations of various natures such as Education, Health, Business and in many other applications. Traditional databases show tremendous performance and are designed to handle structured data with ACID (Atomicity, Consistency, Isolation, Durability) property to manage data integrity. In the current era, organizations are storing more data i.e. videos, images, blogs, etc. besides structured data for decision making. Similarly, social media and scientific applications are generating large amount of semi-structured data of varied nature. Relational databases cannot process properly and manage such large amount of data efficiently. To overcome this problem, another paradigm NoSQL databases is introduced to manage and process massive amount of unstructured data efficiently. NoSQL databases are divided into four categories and each category is used according to the nature and need of the specific problem. In this paper we will compare Oracle relational database and NoSQL graph database using optimized queries and physical database tuning techniques. The comparison is two folded: in the first iteration we compare various kinds of queries such as simpler query, database tuning of Oracle relational database such as sub databases and perform these queries in our desired environments. Secondly, for this comparison we will perform predictive analysis for the results obtained from our experiments.",10.14569/IJACSA.2017.080564
e02acab3e0c44f425705865869f3e51f7998dd24,Towards Temporal Graph Databases,2016,"In spite of the extensive literature on graph databases (GDBs), temporal GDBs have not received too much attention so far. Temporal GBDs can capture, for example, the evolution of social networks across time, a relevant topic in data analysis nowadays. In this paper we propose a data model and query language (denoted TEG-QL) for temporal GDBs, based on the notion of attribute graphs. This allows a straightforward translation to Neo4J, a well-known GBD. We present extensive examples of the use of TEG-QL, and comment our implementation.",
02808823641975868e3212fd7ae859514fdbdf32,Integrity constraints in graph databases,2017,,10.1016/j.procs.2017.05.456
c8df841db2bfe68fc7624f004217c0255f385063,Smart RDF Data Storage in Graph Databases,2017,"Graph Database Management Systems (GDBMs) provide an effective and efficient solution to data storage in current scenarios where data are more and more connected, graph models are widely used, and systems need to scale to large data sets. In particular, the conversion of the persistent layer of an application from a RDF to a graph data store can be convenient but it is usually an hard task for database administrators. In this paper we propose a methodology to convert a RDF data store to a graph database by exploiting the ontology and the constraints of the source. We provide experimental results that show the feasibility of our solution and the efficiency of query answering over the target database.",10.1109/CCGRID.2017.108
0daa156e7b4168df25733ce58680dd15b539fa50,Generating Flexible Workloads for Graph Databases,2016,"Graph data management tools are nowadays evolving at a great pace. Key drivers of progress in the design and study of data intensive systems are solutions for synthetic generation of data and workloads, for use in empirical studies. Current graph generators, however, provide limited or no support for workload generation or are limited to fixed use-cases. Towards addressing these limitations, we demonstrate gMark, the first domain- and query language-independent framework for synthetic graph and query workload generation. Its novel features are: (i) fine-grained control of graph instance and query workload generation via expressive user-defined schemas; (ii) the support of expressive graph query languages, including recursion among other features; and, (iii) selectivity estimation of the generated queries. During the demonstration, we will showcase the highly tunable generation of graphs and queries through various user-defined schemas and targeted selectivities, and the variety of supported practical graph query languages. We will also show a performance comparison of four state-of-the-art graph database engines, which helps us understand their current strengths and desirable future extensions.",10.14778/3007263.3007283
262fe2a2b5d1c0fa7feb63f3bf0c1fd81f1b7c80,Graph Databases: Their Power and Limitations,2015,,10.1007/978-3-319-24369-6_5
acf5fce7472de410985d223d3ce1271a5014d1e7,Using Domain-Specific Languages For Analytic Graph Databases,2016,"Recently graph has been drawing lots of attention both as a natural data model that captures fine-grained relationships between data entities and as a tool for powerful data analysis that considers such relationships. In this paper, we present a new graph database system that integrates a robust graph storage with an efficient graph analytics engine. Primarily, our system adopts two domain-specific languages (DSLs), one for describing graph analysis algorithms and the other for graph pattern matching queries. Compared to the API-based approaches in conventional graph processing systems, the DSL-based approach provides users with more flexible and intuitive ways of expressing algorithms and queries. Moreover, the DSL-based approach has significant performance benefits as well, (1) by skipping (remote) API invocation overhead and (2) by applying high-level optimization from the compiler.",10.14778/3007263.3007265
45af864f2f9991eb9f9545fdec45ccf2adb5f0fa,Graph databases: A survey,2015,"In the era of big data, data analytics, business intelligence database management plays a vital role from technical business management and research point of view. Over many decades, database management has been a topic of active research. There are different type of database management system have been proposed over a period of time but Relational Database Management System (RDBMS) is the one which has been most popularly used in academic research as well as industrial setup[1]. In recent years, graph databases regained interest among the researchers for certain obvious reasons. One of the most important reasons for such an interest in a graph database is because of the inherent property of graphs as a graph structure. Graphs are present everywhere in the data structure, which represents the strong connectivity within the data. Most of the graph database models are defined in which data-structure for schema and instances are modeled as graph or generalization of a graph. In such graph database models, data manipulations are expressed by graph-oriented operations and type constructors [9]. Now days, most of the real world applications can be modeled as a graph and one of the best real world examples is social or biological network. This paper gives an overview of the different type of graph databases, applications, and comparison between their models based on some properties.",10.1109/CCAA.2015.7148480
d6b68954d6acddce319de327af65d164bdfc5543,Supervised Learning on Relational Databases with Graph Neural Networks,2020,"The majority of data scientists and machine learning practitioners use relational data in their work [State of ML and Data Science 2017, Kaggle, Inc.]. But training machine learning models on data stored in relational databases requires significant data extraction and feature engineering efforts. These efforts are not only costly, but they also destroy potentially important relational structure in the data. We introduce a method that uses Graph Neural Networks to overcome these challenges. Our proposed method outperforms state-of-the-art automatic feature engineering methods on two out of three datasets.",
71ca3a6c98c8b60117d01f324487ea8bfe34c54b,Authenticated Subgraph Similarity Searchin Outsourced Graph Databases,2015,"Subgraph similarity search is used in graph databases to retrieve graphs whose subgraphs are similar to a given query graph. It has been proven successful in a wide range of applications including bioinformatics and chem-informatics, etc. Due to the cost of providing efficient similarity search services on ever-increasing graph data, database outsourcing is apparently an appealing solution to database owners. Unfortunately, query service providers may be untrusted or compromised by attacks. To our knowledge, no studies have been carried out on the authentication of the search. In this paper, we propose authentication techniques that follow the popular filtering-and-verification framework. We propose an authentication-friendly metric index called GMTree. Specifically, we transform the similarity search into a search in a graph metric space and derive small verification objects (VOs) to-be-transmitted to query clients. To further optimize GMTree, we propose a sampling-based pivot selection method and an authenticated version of MCS computation. Our comprehensive experiments verified the effectiveness and efficiency of our proposed techniques.",10.1109/TKDE.2014.2316818
342253053f72911edc54df84708f1c1a8db279b6,Graph similarity search on large uncertain graph databases,2015,,10.1007/s00778-014-0373-y
ae63486b2791b301ea4f31b654579687b2f2f95a,SQL2Neo: Moving health-care data from relational to graph databases,2015,"De-facto storage model being used by health-care information systems is Relational Database Management Systems (RDBMS). Albeit relational storage model is mature and widely used; they are incompetent to store and query data encompassing high degree of relationships. Health-care data is heavily annotated with relationships and hence are a suitable candidate for a specialized data model - Graph databases. Graph databases will empower health-care professionals to discover and manage new and useful relationships and also provides speed when querying highly-related data. To query related data, relational databases employ massive joins which are very expensive, in contrast graph data-stores have direct pointers to their adjacent nodes. Hence achieving much needed scalability to handle huge amount of medical data being generated at a very high velocity. Also, healthcare data is primarily semi/un-structured - inciting the need of a schema-less database. In this proposal a methodology to convert a relational to a graph database by exploiting the schema and the constraints of the source, is proposed. The approach supports the translation of conjunctive SQL queries over the source into graph traversal operations over the target. The experimental results are provided to show the feasibility of the solution and the efficiency of query answering over the target database. Tuples are mapped to nodes and foreign key is mapped into edges. Software have been implemented in Java to convert a sample medical relational database with 24 tables to a graph database. During transformation, constraints were preserved. MySQL as relational database and popular graph database - Neo4j was used for the implementation of proposed system - SQL2Neo.",10.1109/IADCC.2015.7154801
f0c9fabba39fb5d471d6bd37f6d1c68811fef48b,DaVinci: Data-driven visual interface construction for subgraph search in graph databases,2015,"Due to the complexity of graph query languages, the need for visual query interfaces that can reduce the burden of query formulation is fundamental to the spreading of graph data management tools to a wider community. Despite the significant progress towards building such query interfaces to simplify visual subgraph query formulation task, construction of current generation visual interfaces is not data-driven. That is, it does not exploit the underlying data graphs to automatically generate the contents of various panels in the interface. Such data-driven construction has several benefits such as superior support for subgraph query formulation and portability of the interface across different graph databases. In this demonstration, we present a novel data-driven visual subgraph query interface construction engine called DaVinci. Specifically, it automatically generates from the underlying database two key components of the visual interface to aid subgraph query formulation, namely canned patterns and node labels.",10.1109/ICDE.2015.7113411
5b0ade1a2ae06e9d8b51f2d5c424a0cdad847e8a,EVALUATION OF GRAPH DATABASES PERFORMANCE THROUGH INDEXING TECHNIQUES,2015,"The aim of this paper is to evaluate , through indexing techniques, the performance of Neo4j and OrientDB, both graph databases technologies and to come up with strength and weaknesses os each technology as a candidate for a storage mechanism of a graph structure. An index is a data structure that makes the searching faster for a specific node in concern of graph databases. The referred data structure is habitually a B-tree, however, can be a hash table or some other logic structure as well. The pivotal point of having an index is to speed up search queries, primarily by reducing the number of nodes in a graph or table to be examined. Graphs and graph databases are more commonly associated with social networking or “graph search” style recommendations. Thus, these technologies remarkably are a core technology platform for some Internet giants like Hi5, Facebook, Google, Badoo, Twitter and LinkedIn. The key to understanding graph database systems, in the social networking context, is they give equal prominence to storing both the data (users, favorites) and the relationships between them (who liked what, who ‘follows’ whom, which post was liked the most, what is the shortest path to ‘reach’ who). By a suitable application case study, in case a Twitter social networking of almost 5,000 nodes imported in local servers (Neo4j and Orient-DB), one queried to retrieval the node with the searched data, first without index (full scan), and second with index, aiming at comparing the response time (statement query time) of the aforementioned graph databases and find out which of them has a better performance (the speed of data or information retrieval) and in which case. Thereof, the main results are presented in the section 6.",10.5121/ijaia.2015.6506
575c8ee3782f37dd0e7df8b9f3161ae3e7783b1c,Query Processing and Optimization in Graph Databases,2015,,
8f6e3b466c4ecc6e0ff779e58cf1b885c7b09f11,Exploiting RDF Open Data Using NoSQL Graph Databases,2015,,10.1007/978-3-319-23868-5_13
2529bb021fda27e1b4012ea62d68c26fd53efcd3,Learning Path Queries on Graph Databases,2015,"We investigate the problem of learning graph queries by exploiting user examples. The input consists of a graph database in which the user has labeled a few nodes as positive or negative examples, depending on whether or not she would like the nodes as part of the query result. Our goal is to handle such examples to find a query whose output is what the user expects. This kind of scenario is pivotal in several application settings where unfamiliar users need to be assisted to specify their queries. In this paper, we focus on path queries defined by regular expressions, we identify fundamental difficulties of our problem setting, we formalize what it means to be learnable, and we prove that the class of queries under study enjoys this property. We additionally investigate an interactive scenario where we start with an empty set of examples and we identify the informative nodes i.e., those that contribute to the learning process. Then, we ask the user to label these nodes and iterate the learning process until she is satisfied with the learned query. Finally, we present an experimental study on both real and synthetic datasets devoted to gauging the effectiveness of our learning algorithm and the improvement of the interactive approach.",10.5441/002/edbt.2015.11
ff784e951325b54f911c3c82bde5b3d6810459c3,Hermes: Dynamic Partitioning for Distributed Social Network Graph Databases,2015,"Social networks are large graphs that require multiple graph database servers to store and manage them. Each database server hosts a graph partition with the objectives of balancing server loads, reducing remote traversals (edge-cuts), and adapting the partitioning to changes in the structure of the graph in the face of changing workloads. To achieve these objectives, a dynamic repartitioning algorithm is required to modify an existing partitioning to maintain good quality partitions while not imposing a significant overhead to the system. In this paper, we introduce a lightweight repartitioner, which dynamically modifies a partitioning using a small amount of resources. In contrast to the existing repartitioning algorithms, our lightweight repartitioner is e cient, making it suitable for use in a real system. We integrated our lightweight repartitioner into Hermes, which we designed as an extension of the open source Neo4j graph database system, to support workloads over partitioned graph data distributed over multiple servers. Using real-world social network data, we show that Hermes leverages the lightweight repartitioner to maintain high quality partitions and provides a 2 to 3 times performance improvement over the de-facto standard random hash-based partitioning.",10.5441/002/edbt.2015.04
2aac97b85bfb33c0bae35be23b02238c79a937d7,Experiences in WordNet Visualization with Labeled Graph Databases,2015,,10.1007/978-3-319-52758-1_6
3b3df8387a7924c966c87662d4d2547e5c0fb662,Extracting Fuzzy Summaries from NoSQL Graph Databases,2015,,10.1007/978-3-319-26154-6_15
217c25a7dc4380f12497edf4d7d013d11195c005,Microblogging Queries on Graph Databases: An Introspection,2015,"Microblogging data is growing at a rapid pace. This poses new challenges to the data management systems, such as graph databases, that are typically suitable for analyzing such data. In this paper, we share our experience on executing a wide variety of micro-blogging queries on two popular graph databases: Neo4j and Sparksee. Our queries are designed to be relevant to popular applications of micro-blogging data. The queries are executed on a large real graph data set comprising of nearly 50 million nodes and 326 million edges.",10.1145/2764947.2764952
0e9882f5d9a4c0ee873f7806ffe645b0473377a7,Interactive Path Query Specification on Graph Databases,2015,"Graph databases are becoming pervasive in several application scenarios such as the Semantic Web, social and biological networks, and geographical databases, to name a few. However, specifying a graph query is a cumbersome task for non-expert users because graph databases (i) are usually of large size hence difficult to visualize and (ii) do not carry proper metadata as there is no clear distinction between the instances and the schemas. We present GPS, a system for interactive path query specification on graph databases, which assists the user to specify path queries defined by regular expressions. The user is interactively asked to visualize small fragments of the graph and to label nodes of interest as positive or negative, depending on whether or not she would like the nodes as part of the query result. After each interaction, the system prunes the uninformative nodes i.e., those that do not add any information about the user's goal query. Thus, the system also guides the user to specify her goal query with a minimal number of interactions.",10.5441/002/dbt.2015.44
0073395c4fec448b21963cf06ab01b671ccaaac3,Kojaph: Visual Definition and Exploration of Patterns in Graph Databases,2015,,10.1007/978-3-319-27261-0_23
a5170eb87547140ee2e9d89e12e49e64b5f1f7d5,Controlling Diversity in Benchmarking Graph Databases,2015,,
205d1d5689f7327d909f6a4addb2f0d98ab7d17e,In-Memory Graph Databases for Web-Scale Data,2015,"A software stack relies primarily on graph-based methods to implement scalable resource description framework databases on top of commodity clusters, providing an inexpensive way to extract meaning from volumes of heterogeneous data.",10.1109/MC.2015.74
6300cd54e6ae992d87cb22ce536aa99a5f9e6ab4,A performance evaluation of open source graph databases,2014,"With the proliferation of large, irregular, and sparse relational datasets, new storage and analysis platforms have arisen to fill gaps in performance and capability left by conventional approaches built on traditional database technologies and query languages. Many of these platforms apply graph structures and analysis techniques to enable users to ingest, update, query, and compute on the topological structure of the network represented as sets of edges relating sets of vertices. To store and process Facebook-scale datasets, software and algorithms must be able to support data sources with billions of edges, update rates of millions of updates per second, and complex analysis kernels. These platforms must provide intuitive interfaces that enable graph experts and novice programmers to write implementations of common graph algorithms. In this paper, we conduct a qualitative study and a performance comparison of 12 open source graph databases using four fundamental graph algorithms on networks containing up to 256 million edges.",10.1145/2567634.2567638
6008ea0ff9406834436c2ab39dca17390f978a23,Model-Driven Design of Graph Databases,2014,,10.1007/978-3-319-12206-9_14
212d1c7cfad4d8dae39deb669337cb46b0274d78,Fuzzy Queries over NoSQL Graph Databases: Perspectives for Extending the Cypher Language,2014,,10.1007/978-3-319-08852-5_40
ccca203382e5dd198c089a0f1d7af7bef0f694e9,TBtools - an integrative toolkit developed for interactive analyses of big biological data.,2020,,10.1016/j.molp.2020.06.009
91b63db746becca15090963a8990dfe2b5103799,"Big data: The next frontier for innovation, competition, and productivity",2011,,
f117c6f12d067bd66dad40996b3931c069daa2da,Business Intelligence and Analytics: From Big Data to Big Impact,2012,"Business intelligence and analytics (BI&A) has emerged as an important area of study for both practitioners and researchers, reflecting the magnitude and impact of data-related problems to be solved in contemporary business organizations. This introduction to the MIS Quarterly Special Issue on Business Intelligence Research first provides a framework that identifies the evolution, applications, and emerging research areas of BI&A. BI&A 1.0, BI&A 2.0, and BI&A 3.0 are defined and described in terms of their key characteristics and capabilities. Current research in BI&A is analyzed and challenges and opportunities associated with BI&A research and education are identified. We also report a bibliometric study of critical BI&A publications, researchers, and research topics based on more than a decade of related academic and industry publications. Finally, the six articles that comprise this special issue are introduced and characterized in terms of the proposed BI&A research framework.",10.2307/41703503
bf5a42b53d156c0811e88e60d2a49f9fd9367cae,Big data: the management revolution.,2012,,
4b06c7e29280b1c6bc05c9df39023b48fef02c93,Escaping the Big Data Paradigm with Compact Transformers,2021,"With the rise of Transformers as the standard for language processing, and their advancements in computer vision, there has been a corresponding growth in parameter size and amounts of training data. Many have come to believe that because of this, transformers are not suitable for small sets of data. This trend leads to concerns such as: limited availability of data in certain scientific domains and the exclusion of those with limited resource from research in the field. In this paper, we aim to present an approach for small-scale learning by introducing Compact Transformers. We show for the first time that with the right size, convolutional tokenization, transformers can avoid overfitting and outperform state-of-the-art CNNs on small datasets. Our models are flexible in terms of model size, and can have as little as 0.28M parameters while achieving competitive results. Our best model can reach 98% accuracy when training from scratch on CIFAR-10 with only 3.7M parameters, which is a significant improvement in data-efficiency over previous Transformer based models being over 10x smaller than other transformers and is 15% the size of ResNet50 while achieving similar performance. CCT also outperforms many modern CNN based approaches, and even some recent NAS-based approaches. Additionally, we obtain a new SOTA result on Flowers-102 with 99.76% top-1 accuracy, and improve upon the existing baseline on ImageNet (82.71% accuracy with 29% as many parameters as ViT), as well as NLP tasks. Our simple and compact design for transformers makes them more feasible to study for those with limited computing resources and/or dealing with small datasets, while extending existing research efforts in data efficient transformers. Our code and pre-trained models are publicly available at https://github.com/SHI-Labs/Compact-Transformers.",
85328b4a8132bf4299f8cd7f8e79e850d561c8fc,Big Data Analytics: A Survey,2022,"Internet-based programs and communication techniques have become widely used and respected in the IT industry recently. A persistent source of ""big data,"" or data that is enormous in volume, diverse in type, and has a complicated multidimensional structure, is internet applications and communications. Today, several measures are routinely performed with no assurance that any of them will be helpful in understanding the phenomenon of interest in an era of automatic, large-scale data collection. Online transactions that involve buying, selling, or even investing are all examples of e-commerce. As a result, they generate data that has a complex structure and a high dimension. The usual data storage techniques cannot handle those enormous volumes of data. There is a lot of work being done to find ways to minimize the dimensionality of big data in order to provide analytics reports that are even more accurate and data visualizations that are more interesting. As a result, the purpose of this survey study is to give an overview of big data analytics along with related problems and issues that go beyond technology.",10.25195/ijci.v49i1.384
41d4e093d5f7ed5aae1aaa9eb6c037742e4cf9b1,The use of Big Data Analytics in healthcare,2022,"The introduction of Big Data Analytics (BDA) in healthcare will allow to use new technologies both in treatment of patients and health management. The paper aims at analyzing the possibilities of using Big Data Analytics in healthcare. The research is based on a critical analysis of the literature, as well as the presentation of selected results of direct research on the use of Big Data Analytics in medical facilities. The direct research was carried out based on research questionnaire and conducted on a sample of 217 medical facilities in Poland. Literature studies have shown that the use of Big Data Analytics can bring many benefits to medical facilities, while direct research has shown that medical facilities in Poland are moving towards data-based healthcare because they use structured and unstructured data, reach for analytics in the administrative, business and clinical area. The research positively confirmed that medical facilities are working on both structural data and unstructured data. The following kinds and sources of data can be distinguished: from databases, transaction data, unstructured content of emails and documents, data from devices and sensors. However, the use of data from social media is lower as in their activity they reach for analytics, not only in the administrative and business but also in the clinical area. It clearly shows that the decisions made in medical facilities are highly data-driven. The results of the study confirm what has been analyzed in the literature that medical facilities are moving towards data-based healthcare, together with its benefits.",10.1186/s40537-021-00553-4
cc017a62c605a0749e35a1264a46d62e78fb68b7,Big Data Analytics,2019,,10.1007/978-3-642-35542-4
1bc34cb22131554ba18f6ba9e6ede5beb42939f1,"Beyond the hype: Big data concepts, methods, and analytics",2015,,10.1016/J.IJINFOMGT.2014.10.007
4e6bba65f7636a655c778a3e54cc58e148468963,CRITICAL QUESTIONS FOR BIG DATA,2012,"The era of Big Data has begun. Computer scientists, physicists, economists, mathematicians, political scientists, bio-informaticists, sociologists, and other scholars are clamoring for access to the massive quantities of information produced by and about people, things, and their interactions. Diverse groups argue about the potential benefits and costs of analyzing genetic sequences, social media interactions, health records, phone logs, government records, and other digital traces left by people. Significant questions emerge. Will large-scale search data help us create better tools, services, and public goods? Or will it usher in a new wave of privacy incursions and invasive marketing? Will data analytics help us understand online communities and political movements? Or will it be used to track protesters and suppress speech? Will it transform how we study human communication and culture, or narrow the palette of research options and alter what ‘research’ means? Given the rise of Big Data as a socio-technical phenomenon, we argue that it is necessary to critically interrogate its assumptions and biases. In this article, we offer six provocations to spark conversations about the issues of Big Data: a cultural, technological, and scholarly phenomenon that rests on the interplay of technology, analysis, and mythology that provokes extensive utopian and dystopian rhetoric.",10.1080/1369118X.2012.678878
38f5b53b49be555430f33b8363910191a3df1d14,"A Survey on Big Data Analytics: Challenges, Open Research Issues and Tools",2022,"Abstract: A huge repository of terabytes of data is generated each day from modern information systems and digital technologies such as Internet of Things and cloud computing. Analysis of these massive data requires a lot of efforts at multiple levels to extract knowledge for decision making. Therefore, big data analysis is a current area of research and development. The basic objective of this paper is to explore the potential impact of big data challenges, open research issues, and various tools associated with it. As a result, this article provides a platform to explore big data at numerous stages. Additionally, it opens a new horizon for researchers to develop the solution, based on the challenges and open research issues.",10.14569/IJACSA.2016.070267
1d174f0e3c391368d0f3384a144a6c7487f2a143,Big Data's Disparate Impact,2016,"Advocates of algorithmic techniques like data mining argue that these techniques eliminate human biases from the decision-making process. But an algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers. In other cases, data may simply reflect the widespread biases that persist in society at large. In still others, data mining can discover surprisingly useful regularities that are really just preexisting patterns of exclusion and inequality. Unthinking reliance on data mining can deny historically disadvantaged and vulnerable groups full participation in society. Worse still, because the resulting discrimination is almost always an unintentional emergent property of the algorithm’s use rather than a conscious choice by its programmers, it can be unusually hard to identify the source of the problem or to explain it to a court.This Essay examines these concerns through the lens of American antidiscrimination law — more particularly, through Title VII’s prohibition of discrimination in employment. In the absence of a demonstrable intent to discriminate, the best doctrinal hope for data mining’s victims would seem to lie in disparate impact doctrine. Case law and the Equal Employment Opportunity Commission’s Uniform Guidelines, though, hold that a practice can be justified as a business necessity when its outcomes are predictive of future employment outcomes, and data mining is specifically designed to find such statistical correlations. Unless there is a reasonably practical way to demonstrate that these discoveries are spurious, Title VII would appear to bless its use, even though the correlations it discovers will often reflect historic patterns of prejudice, others’ discrimination against members of protected groups, or flaws in the underlying dataAddressing the sources of this unintentional discrimination and remedying the corresponding deficiencies in the law will be difficult technically, difficult legally, and difficult politically. There are a number of practical limits to what can be accomplished computationally. For example, when discrimination occurs because the data being mined is itself a result of past intentional discrimination, there is frequently no obvious method to adjust historical data to rid it of this taint. Corrective measures that alter the results of the data mining after it is complete would tread on legally and politically disputed terrain. These challenges for reform throw into stark relief the tension between the two major theories underlying antidiscrimination law: anticlassification and antisubordination. Finding a solution to big data’s disparate impact will require more than best efforts to stamp out prejudice and bias; it will require a wholesale reexamination of the meanings of “discrimination” and “fairness.”",10.2139/SSRN.2477899
3a74bed911ccf213d9595b2b02a5b1c4ac4dcaf8,Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy,2017,,10.1080/01972243.2017.1354593
178571a5cde984c895493e2eb6c5487449d055cf,"Data mining in clinical big data: the frequently used databases, steps, and methodological models",2021,"Many high quality studies have emerged from public databases, such as Surveillance, Epidemiology, and End Results (SEER), National Health and Nutrition Examination Survey (NHANES), The Cancer Genome Atlas (TCGA), and Medical Information Mart for Intensive Care (MIMIC); however, these data are often characterized by a high degree of dimensional heterogeneity, timeliness, scarcity, irregularity, and other characteristics, resulting in the value of these data not being fully utilized. Data-mining technology has been a frontier field in medical research, as it demonstrates excellent performance in evaluating patient risks and assisting clinical decision-making in building disease-prediction models. Therefore, data mining has unique advantages in clinical big-data research, especially in large-scale medical public databases. This article introduced the main medical public database and described the steps, tasks, and models of data mining in simple language. Additionally, we described data-mining methods along with their practical applications. The goal of this work was to aid clinical researchers in gaining a clear and intuitive understanding of the application of data-mining technology on clinical big-data in order to promote the production of research results that are beneficial to doctors and patients.",10.1186/s40779-021-00338-z
92fd5aaeacaa332a725e72647e20baec5c73b73d,Big Data Analytics in Supply Chain Management: A Systematic Literature Review and Research Directions,2022,"Big data analytics has been successfully used for various business functions, such as accounting, marketing, supply chain, and operations. Currently, along with the recent development in machine learning and computing infrastructure, big data analytics in the supply chain are surging in importance. In light of the great interest and evolving nature of big data analytics in supply chains, this study conducts a systematic review of existing studies in big data analytics. This study presents a framework of a systematic literature review from interdisciplinary perspectives. From the organizational perspective, this study examines the theoretical foundations and research models that explain the sustainability and performances achieved through the use of big data analytics. Then, from the technical perspective, this study analyzes types of big data analytics, techniques, algorithms, and features developed for enhanced supply chain functions. Finally, this study identifies the research gap and suggests future research directions.",10.3390/bdcc6010017
b904dcdbd7c7b33938583f2f57d05ca70e121ea9,An Efficient and Secure Big Data Storage in Cloud Environment by Using Triple Data Encryption Standard,2022,"In recent decades, big data analysis has become the most important research topic. Hence, big data security offers Cloud application security and monitoring to host highly sensitive data to support Cloud platforms. However, the privacy and security of big data has become an emerging issue that restricts the organization to utilize Cloud services. The existing privacy preserving approaches showed several drawbacks such as a lack of data privacy and accurate data analysis, a lack of efficiency of performance, and completely rely on third party. In order to overcome such an issue, the Triple Data Encryption Standard (TDES) methodology is proposed to provide security for big data in the Cloud environment. The proposed TDES methodology provides a relatively simpler technique by increasing the sizes of keys in Data Encryption Standard (DES) to protect against attacks and defend the privacy of data. The experimental results showed that the proposed TDES method is effective in providing security and privacy to big healthcare data in the Cloud environment. The proposed TDES methodology showed less encryption and decryption time compared to the existing Intelligent Framework for Healthcare Data Security (IFHDS) method.",10.3390/bdcc6040101
1597449a7f64b6bd24639b4deab96c8a8c184177,"Digital twin-driven product design, manufacturing and service with big data",2017,,10.1007/s00170-017-0233-1
bf69c98fca9a9f6c1cde871beddbcdc668b77771,"Big Data: A Revolution That Will Transform How We Live, Work, and Think",2015,,
f12930cd5f58990badc1a7c5d2749cad004cfb0e,Big data analytics for intelligent manufacturing systems: A review,2021,,10.1016/J.JMSY.2021.03.005
fe44200fed05f9a7c656f2245deded8fd5f5e1e6,CatBoost for big data: an interdisciplinary review,2020,"Gradient Boosted Decision Trees (GBDT’s) are a powerful tool for classification and regression tasks in Big Data. Researchers should be familiar with the strengths and weaknesses of current implementations of GBDT’s in order to use them effectively and make successful contributions. CatBoost is a member of the family of GBDT machine learning ensemble techniques. Since its debut in late 2018, researchers have successfully used CatBoost for machine learning studies involving Big Data. We take this opportunity to review recent research on CatBoost as it relates to Big Data, and learn best practices from studies that cast CatBoost in a positive light, as well as studies where CatBoost does not outshine other techniques, since we can learn lessons from both types of scenarios. Furthermore, as a Decision Tree based algorithm, CatBoost is well-suited to machine learning tasks involving categorical, heterogeneous data. Recent work across multiple disciplines illustrates CatBoost’s effectiveness and shortcomings in classification and regression tasks. Another important issue we expose in literature on CatBoost is its sensitivity to hyper-parameters and the importance of hyper-parameter tuning. One contribution we make is to take an interdisciplinary approach to cover studies related to CatBoost in a single work. This provides researchers an in-depth understanding to help clarify proper application of CatBoost in solving problems. To the best of our knowledge, this is the first survey that studies all works related to CatBoost in a single publication.",10.1186/s40537-020-00369-8
94deb62af3054c49e7d80bd7eb3ed5efe990fc0b,Traffic Flow Prediction With Big Data: A Deep Learning Approach,2015,"Accurate and timely traffic flow information is important for the successful deployment of intelligent transportation systems. Over the last few years, traffic data have been exploding, and we have truly entered the era of big data for transportation. Existing traffic flow prediction methods mainly use shallow traffic prediction models and are still unsatisfying for many real-world applications. This situation inspires us to rethink the traffic flow prediction problem based on deep architecture models with big traffic data. In this paper, a novel deep-learning-based traffic flow prediction method is proposed, which considers the spatial and temporal correlations inherently. A stacked autoencoder model is used to learn generic traffic flow features, and it is trained in a greedy layerwise fashion. To the best of our knowledge, this is the first time that a deep architecture model is applied using autoencoders as building blocks to represent traffic flow features for prediction. Moreover, experiments demonstrate that the proposed method for traffic flow prediction has superior performance.",10.1109/TITS.2014.2345663
b6b7fea1846e85ac1e3c7e3adda6e65b127d0368,"IoT, Big Data, and Artificial Intelligence in Agriculture and Food Industry",2020,"Internet of Things (IoT) results in a massive amount of streaming data, often referred to as “big data,” which brings new opportunities to monitor agricultural and food processes. Besides sensors, big data from social media is also becoming important for the food industry. In this review, we present an overview of IoT, big data, and artificial intelligence (AI), and their disruptive role in shaping the future of agri-food systems. Following an introduction to the fields of IoT, big data, and AI, we discuss the role of IoT and big data analysis in agriculture (including greenhouse monitoring, intelligent farm machines, and drone-based crop imaging), supply chain modernization, social media (for open innovation and sentiment analysis) in food industry, food quality assessment (using spectral methods and sensor fusion), and finally, food safety (using gene sequencing and blockchain-based digital traceability). A special emphasis is laid on the commercial status of applications and translational research outcomes.",10.1109/jiot.2020.2998584
a0d18dddaa995b126ad373e33767b9b881d16b2f,An Introductory Review of Deep Learning for Prediction Models With Big Data,2020,"Deep learning models stand for a new learning paradigm in artificial intelligence (AI) and machine learning. Recent breakthrough results in image analysis and speech recognition have generated a massive interest in this field because also applications in many other domains providing big data seem possible. On a downside, the mathematical and computational methodology underlying deep learning models is very challenging, especially for interdisciplinary scientists. For this reason, we present in this paper an introductory review of deep learning approaches including Deep Feedforward Neural Networks (D-FFNN), Convolutional Neural Networks (CNNs), Deep Belief Networks (DBNs), Autoencoders (AEs), and Long Short-Term Memory (LSTM) networks. These models form the major core architectures of deep learning models currently used and should belong in any data scientist's toolbox. Importantly, those core architectural building blocks can be composed flexibly—in an almost Lego-like manner—to build new application-specific network architectures. Hence, a basic understanding of these network architectures is important to be prepared for future developments in AI.",10.3389/frai.2020.00004
752604994a7ca548ff2954114fc61a501d857b1c,Big data analytics and firm performance: Effects of dynamic capabilities,2017,,10.1016/J.JBUSRES.2016.08.009
8bba999de25bfb288b3f7f88e1d907aab02638b6,Big-Data Science in Porous Materials: Materials Genomics and Machine Learning,2020,"By combining metal nodes with organic linkers we can potentially synthesize millions of possible metal–organic frameworks (MOFs). The fact that we have so many materials opens many exciting avenues but also create new challenges. We simply have too many materials to be processed using conventional, brute force, methods. In this review, we show that having so many materials allows us to use big-data methods as a powerful technique to study these materials and to discover complex correlations. The first part of the review gives an introduction to the principles of big-data science. We show how to select appropriate training sets, survey approaches that are used to represent these materials in feature space, and review different learning architectures, as well as evaluation and interpretation strategies. In the second part, we review how the different approaches of machine learning have been applied to porous materials. In particular, we discuss applications in the field of gas storage and separation, the stability of these materials, their electronic properties, and their synthesis. Given the increasing interest of the scientific community in machine learning, we expect this list to rapidly expand in the coming years.",10.1021/ACS.CHEMREV.0C00004
456c011594ecacdd24298a161787389ccbe4b88b,Big Data Service Architecture: A Survey,2020,,
8b417c2be7a7707f372049fb1193f0d42f799562,Big Data and AI Revolution in Precision Agriculture: Survey and Challenges,2021,"Sustainable agricultural development is a significant solution with fast population development through the use of information and communication (ICT) in precision agriculture, which produced new methods for making cultivation further productive, proficient, well-regulated while preserving the climate. Big data (machine learning, deep learning, etc.) is amongst the vital technologies of ICT employed in precision agriculture for their huge data analytical capabilities to abstract significant information and to assist agricultural practitioners to comprehend well farming practices and take precise decisions. The main goal of this article is to acquire an awareness of the Big Data latest applications in smart agriculture and be acquainted with related social and financial challenges to be concentrated on. This article features data creation methods, accessibility of technology, accessibility of devices, software tools, and data analytic methods, and appropriate applications of big data in precision agriculture. Besides, there are still a few challenges that come across the widespread implementation of big data technology in agriculture.",10.1109/ACCESS.2021.3102227
b34fc78de28be598e21118d7cb9d84d63374addc,Analysis of Dimensionality Reduction Techniques on Big Data,2020,"Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.",10.1109/ACCESS.2020.2980942
02a88547d6f6022bebc9aba6723a310cdf132f3f,Big Data,2014,,10.1007/s11576-014-0434-2
48fc9c42522184c652742255fdf31f7b9ed7ebae,Brief introduction of medical database and data mining technology in big data era,2020,"Data mining technology can search for potentially valuable knowledge from a large amount of data, mainly divided into data preparation and data mining, and expression and analysis of results. It is a mature information processing technology and applies database technology. Database technology is a software science that researches manages, and applies databases. The data in the database are processed and analyzed by studying the underlying theory and implementation methods of the structure, storage, design, management, and application of the database. We have introduced several databases and data mining techniques to help a wide range of clinical researchers better understand and apply database technology.",10.1111/jebm.12373
cbad0923db89f23febcbd6192ff4149289ff2ad9,A survey on data‐efficient algorithms in big data era,2021,"The leading approaches in Machine Learning are notoriously data-hungry. Unfortunately, many application domains do not have access to big data because acquiring data involves a process that is expensive or time-consuming. This has triggered a serious debate in both the industrial and academic communities calling for more data-efficient models that harness the power of artificial learners while achieving good results with less training data and in particular less human supervision. In light of this debate, this work investigates the issue of algorithms’ data hungriness. First, it surveys the issue from different perspectives. Then, it presents a comprehensive review of existing data-efficient methods and systematizes them into four categories. Specifically, the survey covers solution strategies that handle data-efficiency by (i) using non-supervised algorithms that are, by nature, more data-efficient, by (ii) creating artificially more data, by (iii) transferring knowledge from rich-data domains into poor-data domains, or by (iv) altering data-hungry algorithms to reduce their dependency upon the amount of samples, in a way they can perform well in small samples regime. Each strategy is extensively reviewed and discussed. In addition, the emphasis is put on how the four strategies interplay with each other in order to motivate exploration of more robust and data-efficient algorithms. Finally, the survey delineates the limitations, discusses research challenges, and suggests future opportunities to advance the research on data-efficiency in machine learning.",10.1186/s40537-021-00419-9
aca6d5f3866372a4506cf15773ae298f18c3f453,A comprehensive survey of anomaly detection techniques for high dimensional big data,2020,"Anomaly detection in high dimensional data is becoming a fundamental research problem that has various applications in the real world. However, many existing anomaly detection techniques fail to retain sufficient accuracy due to so-called “big data” characterised by high-volume, and high-velocity data generated by variety of sources. This phenomenon of having both problems together can be referred to the “curse of big dimensionality,” that affect existing techniques in terms of both performance and accuracy. To address this gap and to understand the core problem, it is necessary to identify the unique challenges brought by the anomaly detection with both high dimensionality and big data problems. Hence, this survey aims to document the state of anomaly detection in high dimensional big data by representing the unique challenges using a triangular model of vertices: the problem (big dimensionality), techniques/algorithms (anomaly detection), and tools (big data applications/frameworks). Authors’ work that fall directly into any of the vertices or closely related to them are taken into consideration for review. Furthermore, the limitations of traditional approaches and current strategies of high dimensional data are discussed along with recent techniques and applications on big data required for the optimization of anomaly detection.",10.1186/s40537-020-00320-x
efca2a32ce9c7a808c2c3efcc2c3dac032dfc8ea,Big Data and Machine Learning in Health Care.,2018,"Nearly all aspects of modern life are in some way being changed by big data and machine learning. Netflix knows what movies people like to watch and Google knows what people want to know based on their search histories. Indeed, Google has recently begun to replace much of its existing non–machine learning technology with machine learning algorithms, and there is great optimism that these techniques can provide similar improvements across many sectors. It isnosurprisethenthatmedicineisawashwithclaims of revolution from the application of machine learning to big health care data. Recent examples have demonstrated that big data and machine learning can create algorithms that perform on par with human physicians.1 Though machine learning and big data may seem mysterious at first, they are in fact deeply related to traditional statistical models that are recognizable to most clinicians. It is our hope that elucidating these connections will demystify these techniques and provide a set of reasonable expectations for the role of machine learning and big data in health care. Machine learning was originally described as a program that learns to perform a task or make a decision automatically from data, rather than having the behavior explicitlyprogrammed.However,thisdefinitionisverybroad and could cover nearly any form of data-driven approach. For instance, consider the Framingham cardiovascular risk score,whichassignspointstovariousfactorsandproduces a number that predicts 10-year cardiovascular risk. Should this be considered an example of machine learning? The answer might obviously seem to be no. Closer inspection oftheFraminghamriskscorerevealsthattheanswermight not be as obvious as it first seems. The score was originally created2 by fitting a proportional hazards model to data frommorethan5300patients,andsothe“rule”wasinfact learnedentirelyfromdata.Designatingariskscoreasamachine learning algorithm might seem a strange notion, but this example reveals the uncertain nature of the original definition of machine learning. It is perhaps more useful to imagine an algorithm as existing along a continuum between fully human-guided vs fully machine-guided data analysis. To understand the degree to which a predictive or diagnostic algorithm can said to be an instance of machine learning requires understanding how much of its structure or parameters were predetermined by humans. The trade-off between human specificationofapredictivealgorithm’spropertiesvslearning those properties from data is what is known as the machine learning spectrum. Returning to the Framingham study, to create the original risk score statisticians and clinical experts worked together to make many important decisions, such as which variables to include in the model, therelationshipbetweenthedependentandindependent variables, and variable transformations and interactions. Since considerable human effort was used to define these properties, it would place low on the machine learning spectrum (#19 in the Figure and Supplement). Many evidence-based clinical practices are based on a statistical model of this sort, and so many clinical decisions in fact exist on the machine learning spectrum (middle left of Figure). On the extreme low end of the machine learning spectrum would be heuristics and rules of thumb that do not directly involve the use of any rules or models explicitly derived from data (bottom left of Figure). Suppose a new cardiovascular risk score is created that includes possible extensions to the original model. For example, it could be that risk factors should not be added but instead should be multiplied or divided, or perhaps a particularly important risk factor should square the entire score if it is present. Moreover, if it is not known in advance which variables will be important, but thousands of individual measurements have been collected, how should a good model be identified from among the infinite possibilities? This is precisely what a machine learning algorithm attempts to do. As humans impose fewer assumptions on the algorithm, it moves further up the machine learning spectrum. However, there is never a specific threshold wherein a model suddenly becomes “machine learning”; rather, all of these approaches exist along a continuum, determined by how many human assumptions are placed onto the algorithm. An example of an approach high on the machine learning spectrum has recently emerged in the form of so-called deep learning models. Deep learning models are stunningly complex networks of artificial neurons that were designed expressly to create accurate models directly from raw data. Researchers recently demonstrated a deep learning algorithm capable of detecting diabetic retinopathy (#4 in the Figure, top center) from retinal photographs at a sensitivity equal to or greater than that of ophthalmologists.1 This model learned the diagnosis procedure directly from the raw pixels of the images with no human intervention outside of a team of ophthalmologists who annotated each image with the correct diagnosis. Because they are able to learn the task with little human instruction or prior assumptions, these deep learning algorithms rank very high on the machine learning spectrum (Figure, light blue circles). Though they require less human guidance, deep learning algorithms for image recognition require enormous amounts of data to capture the full complexity, variety, and nuance inherent to real-world images. Consequently, these algorithms often require hundreds of thousands of examples to extract the salient image features that are correlated with the outcome of interest. Higher placement on the machine learning spectrum does not imply superiority, because different tasks require different levels of human involvement. While algorithms high on the spectrum are often very flexible and can learn many tasks, they are often uninterpretable VIEWPOINT",10.1001/jama.2017.18391
391a5f286f814d852dddcab1b2b68e5c1af6c79e,Data mining with big data,2016,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This paper presents a HACE theorem that characterizes the features of the Big Data revolution, and proposes a Big Data processing model, from the data mining perspective. This data-driven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the data-driven model and also in the Big Data revolution.",10.1109/TKDE.2013.109
933baeec555352784848a93284c9dd0e79477759,Big Data in Smart Farming – A review,2017,,10.1016/J.AGSY.2017.01.023
b473e91cbe80c8b46451b49153cd5f93030480ab,Critical analysis of Big Data challenges and analytical methods,2017,,10.1016/J.JBUSRES.2016.08.001
56266342b01a4f2ddc28a1e8401dbbad105736a5,Big Data Analytics in Weather Forecasting: A Systematic Review,2021,,10.1007/s11831-021-09616-4
73d4accea441aae2373828a8dc2175aa2759c38f,Big Data in Finance,2021,"  Big data is revolutionizing the finance industry and has the potential to significantly shape future research in finance. This special issue contains papers following the 2019 NBER-RFS Conference on Big Data. In this introduction to the special issue, we define the “big data” phenomenon as a combination of three features: large size, high dimension, and complex structure. Using the papers in the special issue, we discuss how new research builds on these features to push the frontier on fundamental questions across areas in finance—including corporate finance, market microstructure, and asset pricing. Finally, we offer some thoughts for future research directions.",10.3386/W28615
4d1fdd81f033cd58f3723bfc61e7d12079647a7a,"Predicting the Future - Big Data, Machine Learning, and Clinical Medicine.",2016,"The algorithms of machine learning, which can sift through vast numbers of variables looking for combinations that reliably predict outcomes, will improve prognosis, displace much of the work of radiologists and anatomical pathologists, and improve diagnostic accuracy.",10.1056/NEJMp1606181
37b0a7a6c8fb26e32b9206847e78d521a2cd5900,A literature review on one-class classification and its potential applications in big data,2021,"In severely imbalanced datasets, using traditional binary or multi-class classification typically leads to bias towards the class(es) with the much larger number of instances. Under such conditions, modeling and detecting instances of the minority class is very difficult. One-class classification (OCC) is an approach to detect abnormal data points compared to the instances of the known class and can serve to address issues related to severely imbalanced datasets, which are especially very common in big data. We present a detailed survey of OCC-related literature works published over the last decade, approximately. We group the different works into three categories: outlier detection, novelty detection, and deep learning and OCC. We closely examine and evaluate selected works on OCC such that a good cross section of approaches, methods, and application domains is represented in the survey. Commonly used techniques in OCC for outlier detection and for novelty detection, respectively, are discussed. We observed one area that has been largely omitted in OCC-related literature is its application context for big data and its inherently associated problems, such as severe class imbalance, class rarity, noisy data, feature selection, and data reduction. We feel the survey will be appreciated by researchers working in these areas of big data.",10.1186/s40537-021-00514-x
733fc094e785724621c46e20db1be69f132ad9df,Comprehensive Survey of Big Data Mining Approaches in Cloud Systems,2021,"Cloud computing, data mining, and big online data are discussed in this paper as hybridization possibilities. The method of analyzing and visualizing vast volumes of data is known as the visualization of data mining. The effect of computing conventions and algorithms on detailed storage and data communication requirements has been studied. When researching these approaches to data storage in big data, the data analytical viewpoint is often explored. These terminology and aspects have been used to address methodological development as well as problem statements. This will assist in the investigation of computational capacity as well as new knowledge in this area. The patterns of using big data were compared in many articles. In this paper, we research Big Data Mining Approaches in Cloud Systems and address cloudcompatible problems and computing techniques to promote Big Data Mining in Cloud Systems. Keywords— Big Data, Data Computing, Big Data Mining,",10.48161/QAJ.V1N2A46
9198d6ab8ad1c6f1527a7d4adf06809848bf23c7,Challenges and Future Directions of Big Data and Artificial Intelligence in Education,2020,"We discuss the new challenges and directions facing the use of big data and artificial intelligence (AI) in education research, policy-making, and industry. In recent years, applications of big data and AI in education have made significant headways. This highlights a novel trend in leading-edge educational research. The convenience and embeddedness of data collection within educational technologies, paired with computational techniques have made the analyses of big data a reality. We are moving beyond proof-of-concept demonstrations and applications of techniques, and are beginning to see substantial adoption in many areas of education. The key research trends in the domains of big data and AI are associated with assessment, individualized learning, and precision education. Model-driven data analytics approaches will grow quickly to guide the development, interpretation, and validation of the algorithms. However, conclusions from educational analytics should, of course, be applied with caution. At the education policy level, the government should be devoted to supporting lifelong learning, offering teacher education programs, and protecting personal data. With regard to the education industry, reciprocal and mutually beneficial relationships should be developed in order to enhance academia-industry collaboration. Furthermore, it is important to make sure that technologies are guided by relevant theoretical frameworks and are empirically tested. Lastly, in this paper we advocate an in-depth dialog between supporters of “cold” technology and “warm” humanity so that it can lead to greater understanding among teachers and students about how technology, and specifically, the big data explosion and AI revolution can bring new opportunities (and challenges) that can be best leveraged for pedagogical practices and learning.",10.3389/fpsyg.2020.580820
8f0d7df1e34867682d0816a38ef4a9bf4a74509c,Big data quality framework: a holistic approach to continuous quality management,2021,,10.1186/s40537-021-00468-0
78e40584f0d149bf6f98beb5561b7b83cb68e1b1,Assessing the impact of big data on firm innovation performance: Big data is not always better data,2020,,10.1016/j.jbusres.2019.09.062
16575f23ff879e6353a55bbfbbcc54e27606bfc5,Big data analytics: Understanding its capabilities and potential benefits for healthcare organizations,2018,,10.1016/J.TECHFORE.2015.12.019
80dd97954ddf3edd22d4cb21f0ac31b7ffed6bbf,Digital Twin and Big Data Towards Smart Manufacturing and Industry 4.0: 360 Degree Comparison,2018,"With the advances in new-generation information technologies, especially big data and digital twin, smart manufacturing is becoming the focus of global manufacturing transformation and upgrading. Intelligence comes from data. Integrated analysis for the manufacturing big data is beneficial to all aspects of manufacturing. Besides, the digital twin paves a way for the cyber-physical integration of manufacturing, which is an important bottleneck to achieve smart manufacturing. In this paper, the big data and digital twin in manufacturing are reviewed, including their concept as well as their applications in product design, production planning, manufacturing, and predictive maintenance. On this basis, the similarities and differences between big data and digital twin are compared from the general and data perspectives. Since the big data and digital twin can be complementary, how they can be integrated to promote smart manufacturing are discussed.",10.1109/ACCESS.2018.2793265
10d89b13a6309a531c35701d37d3bd76a27a3942,Big Data Storage,2021,"This chapter provides an overview of big data storage technologies. It is the result of a survey of the current state of the art in data storage technologies in order to create a cross-sectorial technology roadmap. This chapter provides a concise overview of big data storage systems that are capable of dealing with high velocity, high volumes, and high varieties of data. It describes distributed file systems, NoSQL databases, graph databases, and NewSQL databases. The chapter investigates the challenge of storing data in a secure and privacy-preserving way. The social and economic impact of big data storage technologies is described, open research challenges highlighted, and three selected case studies are provided from the health, finance, and energy sector. Some of the key insights on big data storage are (1) in-memory databases and columnar databases typically outperform traditional relational database systems, (2) the major technical barrier to widespread up-take of big data storage solutions are missing standards, and (3) there is a need to address open research challenges related to the scalability and performance of graph databases.",10.1007/978-3-319-21569-3_7
bc6dbcaf4d2c76e618ae3f1043fd7276cbdf7f9b,"Big data in healthcare: management, analysis and future prospects",2019,"‘Big data’ is massive amounts of information that can work wonders. It has become a topic of special interest for the past two decades because of a great potential that is hidden in it. Various public and private sector industries generate, store, and analyze big data with an aim to improve the services they provide. In the healthcare industry, various sources for big data include hospital records, medical records of patients, results of medical examinations, and devices that are a part of internet of things. Biomedical research also generates a significant portion of big data relevant to public healthcare. This data requires proper management and analysis in order to derive meaningful information. Otherwise, seeking solution by analyzing big data quickly becomes comparable to finding a needle in the haystack. There are various challenges associated with each step of handling big data which can only be surpassed by using high-end computing solutions for big data analysis. That is why, to provide relevant solutions for improving public health, healthcare providers are required to be fully equipped with appropriate infrastructure to systematically generate and analyze big data. An efficient management, analysis, and interpretation of big data can change the game by opening new avenues for modern healthcare. That is exactly why various industries, including the healthcare industry, are taking vigorous steps to convert this potential into better services and financial advantages. With a strong integration of biomedical and healthcare data, modern healthcare organizations can possibly revolutionize the medical therapies and personalized medicine.",10.1186/s40537-019-0217-0
dac7344737cb824634f757aede2dd46a6eed204b,Big data analytics in healthcare: promise and potential,2014,"To describe the promise and potential of big data analytics in healthcare. The paper describes the nascent field of big data analytics in healthcare, discusses the benefits, outlines an architectural framework and methodology, describes examples reported in the literature, briefly discusses the challenges, and offers conclusions. The paper provides a broad overview of big data analytics for healthcare researchers and practitioners. Big data analytics in healthcare is evolving into a promising field for providing insight from very large data sets and improving outcomes while reducing costs. Its potential is great; however there remain challenges to overcome.",10.1186/2047-2501-2-3
1e4709c0b8fe3bf759cd64dc1ede695d6e5316f0,Deep learning applications and challenges in big data analytics,2015,"Big Data Analytics and Deep Learning are two high-focus of data science. Big Data has become important as many organizations both public and private have been collecting massive amounts of domain-specific information, which can contain useful information about problems such as national intelligence, cyber security, fraud detection, marketing, and medical informatics. Companies such as Google and Microsoft are analyzing large volumes of data for business analysis and decisions, impacting existing and future technology. Deep Learning algorithms extract high-level, complex abstractions as data representations through a hierarchical learning process. Complex abstractions are learnt at a given level based on relatively simpler abstractions formulated in the preceding level in the hierarchy. A key benefit of Deep Learning is the analysis and learning of massive amounts of unsupervised data, making it a valuable tool for Big Data Analytics where raw data is largely unlabeled and un-categorized. In the present study, we explore how Deep Learning can be utilized for addressing some important problems in Big Data Analytics, including extracting complex patterns from massive volumes of data, semantic indexing, data tagging, fast information retrieval, and simplifying discriminative tasks. We also investigate some aspects of Deep Learning research that need further exploration to incorporate specific challenges introduced by Big Data Analytics, including streaming data, high-dimensional data, scalability of models, and distributed computing. We conclude by presenting insights into relevant future works by posing some questions, including defining data sampling criteria, domain adaptation modeling, defining criteria for obtaining useful data abstractions, improving semantic indexing, semi-supervised learning, and active learning.",10.1186/s40537-014-0007-7
78aab73ed574393ab421f25b3a0e3f7343e64748,Big Data and Big Data Analytics,2022,,10.1007/978-981-16-3607-3_1
3b217403302f9cb9d9685404c7646de7bc0db428,"Data-intensive applications, challenges, techniques and technologies: A survey on Big Data",2014,,10.1016/j.ins.2014.01.015
718b5a40dba91bfa0bdfb9ac9ca4381425d2ff95,Axes of a revolution: challenges and promises of big data in healthcare,2020,,10.1038/s41591-019-0727-5
f11cba099b8cf14815f7b3d85f55ecfddbf9f04d,An Overview of End-to-End Entity Resolution for Big Data,2020,"One of the most critical tasks for improving data quality and increasing the reliability of data analytics is Entity Resolution (ER), which aims to identify different descriptions that refer to the same real-world entity. Despite several decades of research, ER remains a challenging problem. In this survey, we highlight the novel aspects of resolving Big Data entities when we should satisfy more than one of the Big Data characteristics simultaneously (i.e., Volume and Velocity with Variety). We present the basic concepts, processing steps, and execution strategies that have been proposed by database, semantic Web, and machine learning communities in order to cope with the loose structuredness, extreme diversity, high speed, and large scale of entity descriptions used by real-world applications. We provide an end-to-end view of ER workflows for Big Data, critically review the pros and cons of existing methods, and conclude with the main open research directions.",10.1145/3418896
b52db9e41e15f76bdcfbe674abe0314af545c430,The Rise of “Big Data” on Cloud Computing,2021,,10.1201/9781003032328-3
de6cf3534f39748a223b9bb2b59d2e7ffcb6ae03,Using Big Data to Emulate a Target Trial When a Randomized Trial Is Not Available.,2016,"Ideally, questions about comparative effectiveness or safety would be answered using an appropriately designed and conducted randomized experiment. When we cannot conduct a randomized experiment, we analyze observational data. Causal inference from large observational databases (big data) can be viewed as an attempt to emulate a randomized experiment-the target experiment or target trial-that would answer the question of interest. When the goal is to guide decisions among several strategies, causal analyses of observational data need to be evaluated with respect to how well they emulate a particular target trial. We outline a framework for comparative effectiveness research using big data that makes the target trial explicit. This framework channels counterfactual theory for comparing the effects of sustained treatment strategies, organizes analytic approaches, provides a structured process for the criticism of observational studies, and helps avoid common methodologic pitfalls.",10.1093/aje/kwv254
e449b9b3fe04fe260731a3c74d2123bf6eaadf5b,A Review of Local Outlier Factor Algorithms for Outlier Detection in Big Data Streams,2020,"Outlier detection is a statistical procedure that aims to find suspicious events or items that are different from the normal form of a dataset. It has drawn considerable interest in the field of data mining and machine learning. Outlier detection is important in many applications, including fraud detection in credit card transactions and network intrusion detection. There are two general types of outlier detection: global and local. Global outliers fall outside the normal range for an entire dataset, whereas local outliers may fall within the normal range for the entire dataset, but outside the normal range for the surrounding data points. This paper addresses local outlier detection. The best-known technique for local outlier detection is the Local Outlier Factor (LOF), a density-based technique. There are many LOF algorithms for a static data environment; however, these algorithms cannot be applied directly to data streams, which are an important type of big data. In general, local outlier detection algorithms for data streams are still deficient and better algorithms need to be developed that can effectively analyze the high velocity of data streams to detect local outliers. This paper presents a literature review of local outlier detection algorithms in static and stream environments, with an emphasis on LOF algorithms. It collects and categorizes existing local outlier detection algorithms and analyzes their characteristics. Furthermore, the paper discusses the advantages and limitations of those algorithms and proposes several promising directions for developing improved local outlier detection methods for data streams.",10.3390/bdcc5010001
2660dcf5bd16d14862a7bbb241fa4d85ae34327f,"The rise of ""big data"" on cloud computing: Review and open research issues",2015,,10.1016/J.IS.2014.07.006
3dfa820702b6181c9964931f0a4d47fd298bf429,Mining Big Data in Education: Affordances and Challenges,2020,"The emergence of big data in educational contexts has led to new data-driven approaches to support informed decision making and efforts to improve educational effectiveness. Digital traces of student behavior promise more scalable and finer-grained understanding and support of learning processes, which were previously too costly to obtain with traditional data sources and methodologies. This synthetic review describes the affordances and applications of microlevel (e.g., clickstream data), mesolevel (e.g., text data), and macrolevel (e.g., institutional data) big data. For instance, clickstream data are often used to operationalize and understand knowledge, cognitive strategies, and behavioral processes in order to personalize and enhance instruction and learning. Corpora of student writing are often analyzed with natural language processing techniques to relate linguistic features to cognitive, social, behavioral, and affective processes. Institutional data are often used to improve student and administrational decision making through course guidance systems and early-warning systems. Furthermore, this chapter outlines current challenges of accessing, analyzing, and using big data. Such challenges include balancing data privacy and protection with data sharing and research, training researchers in educational data science methodologies, and navigating the tensions between explanation and prediction. We argue that addressing these challenges is worthwhile given the potential benefits of mining big data in education.",10.3102/0091732X20903304
82870bc488b57cdf5ea62877109a7278af2926b3,Big Data and Artificial Intelligence Modeling for Drug Discovery.,2020,"Due to the massive data sets available for drug candidates, modern drug discovery has advanced to the big data era. Central to this shift is the development of artificial intelligence approaches to implementing innovative modeling based on the dynamic, heterogeneous, and large nature of drug data sets. As a result, recently developed artificial intelligence approaches such as deep learning and relevant modeling studies provide new solutions to efficacy and safety evaluations of drug candidates based on big data modeling and analysis. The resulting models provided deep insights into the continuum from chemical structure to in vitro, in vivo, and clinical outcomes. The relevant novel data mining, curation, and management techniques provided critical support to recent modeling studies. In summary, the new advancement of artificial intelligence in the big data era has paved the road to future rational drug development and optimization, which will have a significant impact on drug discovery procedures and, eventually, public health. Expected final online publication date for the Annual Review of Psychology, Volume 71 is January 4, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.",10.1146/annurev-pharmtox-010919-023324
3789eb72c32ecf5e33442570358dd786dd67c8a2,Text Mining in Big Data Analytics,2020,"Text mining in big data analytics is emerging as a powerful tool for harnessing the power of unstructured textual data by analyzing it to extract new knowledge and to identify significant patterns and correlations hidden in the data. This study seeks to determine the state of text mining research by examining the developments within published literature over past years and provide valuable insights for practitioners and researchers on the predominant trends, methods, and applications of text mining research. In accordance with this, more than 200 academic journal articles on the subject are included and discussed in this review; the state-of-the-art text mining approaches and techniques used for analyzing transcripts and speeches, meeting transcripts, and academic journal articles, as well as websites, emails, blogs, and social media platforms, across a broad range of application areas are also investigated. Additionally, the benefits and challenges related to text mining are also briefly outlined.",10.3390/bdcc4010001
17fca92ffd527c78c5dc6c7953e96671743807fa,Big Data Analytics Capabilities and Innovation: The Mediating Role of Dynamic Capabilities and Moderating Effect of the Environment,2019,"With big data analytics growing rapidly in popularity, academics and practitioners have been considering the means through which they can incorporate the shifts these technologies bring into their competitive strategies. Drawing on the resource&#8208;based view, the dynamic capabilities view, and on recent literature on big data analytics, this study examines the indirect relationship between a big data analytics capability (BDAC) and two types of innovation capabilities: incremental and radical. The study extends existing research by proposing that BDACs enable firms to generate insight that can help strengthen their dynamic capabilities, which in turn positively impact incremental and radical innovation capabilities. To test their proposed research model, the authors used survey data from 175 chief information officers and IT managers working in Greek firms. By means of partial least squares structural equation modelling, the results confirm the authors&#8217; assumptions regarding the indirect effect that BDACs have on innovation capabilities. Specifically, they find that dynamic capabilities fully mediate the effect on both incremental and radical innovation capabilities. In addition, under conditions of high environmental heterogeneity, the impact of BDACs on dynamic capabilities and, in sequence, incremental innovation capability is enhanced, while under conditions of high environmental dynamism the effect of dynamic capabilities on incremental innovation capabilities is amplified.",10.1111/1467-8551.12343
c48e0bd0f36c25ab83befbc7b7da369b75fd25f5,Big Data-Survey,2016,"Big data is the term for any gathering of information sets, so expensive and complex, that it gets to be hard to process for utilizing customary information handling applications. The difficulties incorporate investigation, catch, duration, inquiry, sharing, stockpiling, Exchange, perception, and protection infringement. To reduce spot business patterns, anticipate diseases, conflict etc., we require bigger data sets when compared with the smaller data sets. Enormous information is hard to work with utilizing most social database administration frameworks and desktop measurements and perception bundles, needing rather enormously parallel programming running on tens, hundreds, or even a large number of servers. In this paper there was an observation on Hadoop architecture, different tools used for big data and its security issues.",10.11591/IJEEI.V4I1.195
247dec05283a1a521f99253a6cca6a5858cac0d2,"Big Data and Predictive Analytics and Manufacturing Performance: Integrating Institutional Theory, Resource‐Based View and Big Data Culture",2019,"The importance of big data and predictive analytics has been at the forefront of research for operations and manufacturing management. The literature has reported the influence of big data and predictive analytics for improved supply chain and operational performance, but there has been a paucity of literature regarding the role of external institutional pressures on the resources of the organization to build big data capability. To address this gap, this paper draws on the resource‐based view of the firm, institutional theory and organizational culture to develop and test a model that describes the importance of resources for building capabilities, skills and big data culture and subsequently improving cost and operational performance. We test our research hypotheses using 195 surveys, gathered using a pre‐tested questionnaire. Our contribution lies in providing insights regarding the role of external pressures on the selection of resources under the moderating effect of big data culture and their utilization for capability building, and how this capability affects cost and operational performance.",10.1111/1467-8551.12355
8894d431a768a35dc7ca4d762ebdba4f407b978c,The ProteomeXchange consortium in 2020: enabling ‘big data’ approaches in proteomics,2019,"Abstract The ProteomeXchange (PX) consortium of proteomics resources (http://www.proteomexchange.org) has standardized data submission and dissemination of mass spectrometry proteomics data worldwide since 2012. In this paper, we describe the main developments since the previous update manuscript was published in Nucleic Acids Research in 2017. Since then, in addition to the four PX existing members at the time (PRIDE, PeptideAtlas including the PASSEL resource, MassIVE and jPOST), two new resources have joined PX: iProX (China) and Panorama Public (USA). We first describe the updated submission guidelines, now expanded to include six members. Next, with current data submission statistics, we demonstrate that the proteomics field is now actively embracing public open data policies. At the end of June 2019, more than 14 100 datasets had been submitted to PX resources since 2012, and from those, more than 9 500 in just the last three years. In parallel, an unprecedented increase of data re-use activities in the field, including ‘big data’ approaches, is enabling novel research and new data resources. At last, we also outline some of our future plans for the coming years.",10.1093/nar/gkz984
4e13a8e8ba8d33e15ed037bfca7c651047533990,Big data for cyber physical systems in industry 4.0: a survey,2019,"ABSTRACT With the technology development in cyber physical systems and big data, there are huge potential to apply them to achieve personalization and improve resource efficiency in Industry 4.0. As Industry 4.0 is the relatively new concept originated from an advanced manufacturing vision supported by the German government in 2011, there are only several existing surveys on either cyber physical systems or big data in Industry 4.0. In addition, there are much less surveys related to the intersection between cyber physical systems and big data in Industry 4.0. However, cyber physical systems are closely related to big data in nature. For example, cyber physical systems will continuously generate a large amount of data which requires the big data techniques to process and help to improve system scalability, security, and efficiency. Therefore, we conduct this survey to bring more attention to this critical intersection and highlight the future research direction to achieve the fully autonomy in Industry 4.0.",10.1080/17517575.2018.1442934
dbabab9bf5955558f73a37644f4bb626106a6d73,Big Data Analytics in Intelligent Transportation Systems: A Survey,2019,"Big data is becoming a research focus in intelligent transportation systems (ITS), which can be seen in many projects around the world. Intelligent transportation systems will produce a large amount of data. The produced big data will have profound impacts on the design and application of intelligent transportation systems, which makes ITS safer, more efficient, and profitable. Studying big data analytics in ITS is a flourishing field. This paper first reviews the history and characteristics of big data and intelligent transportation systems. The framework of conducting big data analytics in ITS is discussed next, where the data source and collection methods, data analytics methods and platforms, and big data analytics application categories are summarized. Several case studies of big data analytics applications in intelligent transportation systems, including road traffic accidents analysis, road traffic flow prediction, public transportation service plan, personal travel route plan, rail transportation management and control, and assets maintenance are introduced. Finally, this paper discusses some open challenges of using big data analytics in ITS.",10.1109/TITS.2018.2815678
9e3816be8cf4821d74e258de10ee471382936a30,Privacy in the age of medical big data,2019,,10.1038/s41591-018-0272-7
5bc511aa30f72720260d792e57537379fb04c395,Sentiment Analysis in Tourism: Capitalizing on Big Data,2019,"Advances in technology have fundamentally changed how information is produced and consumed by all actors involved in tourism. Tourists can now access different sources of information, and they can generate their own content and share their views and experiences. Tourism content shared through social media has become a very influential information source that impacts tourism in terms of both reputation and performance. However, the volume of data on the Internet has reached a level that makes manual processing almost impossible, demanding new analytical approaches. Sentiment analysis is rapidly emerging as an automated process of examining semantic relationships and meaning in reviews. In this article, different sentiment analysis approaches applied in tourism are reviewed and assessed in terms of the datasets used and performances on key evaluation metrics. The article concludes by outlining future research avenues to further advance sentiment analysis in tourism as part of a broader Big Data approach.",10.1177/0047287517747753
0e33833f5e2e2719edfba1d142eb4d27f96e799f,Big data analytics and enterprises: a bibliometric synthesis of the literature,2020,"ABSTRACT Understanding the developmental trajectories of big data analytics in the corporate context is highly relevant for information systems research and practice. This study presents a comprehensive bibliometric analysis of applications of big data analytics in enterprises. The sample for this study contained a total of 1727 articles from the Scopus database. The sample was analyzed with techniques such as bibliographic coupling, citation analysis, co-word analysis, and co-authorship analysis. Findings from the co-citation analysis identified four major thematic areas in the extant literature. The evolution of these thematic areas was documented with dynamic co-citation analysis.",10.1080/17517575.2020.1734241
b080d072cfde697180db3234da08903c092e72c3,Big data analytics capabilities and knowledge management: impact on firm performance,2019," Purpose Big data analytics (BDA) guarantees that data may be analysed and categorised into useful information for businesses and transformed into big data related-knowledge and efficient decision-making processes, thereby improving performance. However, the management of the knowledge generated from the BDA as well as its integration and combination with firm knowledge have scarcely been investigated, despite an emergent need of a structured and integrated approach. The paper aims to discuss these issues.   Design/methodology/approach Through an empirical analysis based on structural equation modelling with data collected from 88 Italian SMEs, the authors tested if BDA capabilities have a positive impact on firm performances, as well as the mediator effect of knowledge management (KM) on this relationship.   Findings The findings of this paper show that firms that developed more BDA capabilities than others, both technological and managerial, increased their performances and that KM orientation plays a significant role in amplifying the effect of BDA capabilities.   Originality/value BDA has the potential to change the way firms compete through better understanding, processing, and exploiting of huge amounts of data coming from different internal and external sources and processes. Some managerial and theoretical implications are proposed and discussed in light of the emergence of this new phenomenon. ",10.1108/MD-07-2018-0825
d2ae65522ac50b7b68462f43e5774ff323c52421,"Uncertainty in big data analytics: survey, opportunities, and challenges",2019,"Big data analytics has gained wide attention from both academia and industry as the demand for understanding trends in massive datasets increases. Recent developments in sensor networks, cyber-physical systems, and the ubiquity of the Internet of Things (IoT) have increased the collection of data (including health care, social media, smart cities, agriculture, finance, education, and more) to an enormous scale. However, the data collected from sensors, social media, financial records, etc. is inherently uncertain due to noise, incompleteness, and inconsistency. The analysis of such massive amounts of data requires advanced analytical techniques for efficiently reviewing and/or predicting future courses of action with high precision and advanced decision-making strategies. As the amount, variety, and speed of data increases, so too does the uncertainty inherent within, leading to a lack of confidence in the resulting analytics process and decisions made thereof. In comparison to traditional data techniques and platforms, artificial intelligence techniques (including machine learning, natural language processing, and computational intelligence) provide more accurate, faster, and scalable results in big data analytics. Previous research and surveys conducted on big data analytics tend to focus on one or two techniques or specific application domains. However, little work has been done in the field of uncertainty when applied to big data analytics as well as in the artificial intelligence techniques applied to the datasets. This article reviews previous work in big data analytics and presents a discussion of open challenges and future directions for recognizing and mitigating uncertainty in this domain.",10.1186/s40537-019-0206-3
c24d47ff95cd4bda073c75ec24ececaa3b10c995,A survey of data partitioning and sampling methods to support big data analysis,2020,"Computer clusters with the shared-nothing architecture are the major computing platforms for big data processing and analysis. In cluster computing, data partitioning and sampling are two fundamental strategies to speed up the computation of big data and increase scalability. In this paper, we present a comprehensive survey of the methods and techniques of data partitioning and sampling with respect to big data processing and analysis. We start with an overview of the mainstream big data frameworks on Hadoop clusters. The basic methods of data partitioning are then discussed including three classical horizontal partitioning schemes: range, hash, and random partitioning. Data partitioning on Hadoop clusters is also discussed with a summary of new strategies for big data partitioning, including the new Random Sample Partition (RSP) distributed model. The classical methods of data sampling are then investigated, including simple random sampling, stratified sampling, and reservoir sampling. Two common methods of big data sampling on computing clusters are also discussed: record-level sampling and block-level sampling. Record-level sampling is not as efficient as block-level sampling on big distributed data. On the other hand, block-level sampling on data blocks generated with the classical data partitioning methods does not necessarily produce good representative samples for approximate computing of big data. In this survey, we also summarize the prevailing strategies and related work on sampling-based approximation on Hadoop clusters. We believe that data partitioning and sampling should be considered together to build approximate cluster computing frameworks that are reliable in both the computational and statistical respects.",10.26599/BDMA.2019.9020015
e8b7a9be9f2d0578a95319ed5841978e10429967,Big data management in the mining industry,2020,,10.1007/s12613-019-1937-z
a60a4e5f7f872b9825ddff5d379857c2091ca52b,Current landscape and influence of big data on finance,2020,"Big data is one of the most recent business and technical issues in the age of technology. Hundreds of millions of events occur every day. The financial field is deeply involved in the calculation of big data events. As a result, hundreds of millions of financial transactions occur in the financial world each day. Therefore, financial practitioners and analysts consider it an emerging issue of the data management and analytics of different financial products and services. Also, big data has significant impacts on financial products and services. Therefore, identifying the financial issues where big data has a significant influence is also an important issue to explore with the influences. Based on these concepts, the objective of this paper was to show the current landscape of finance dealing with big data, and also to show how big data influences different financial sectors, more specifically, its impact on financial markets, financial institutions, and the relationship with internet finance, financial management, internet credit service companies, fraud detection, risk analysis, financial application management, and so on. The connection between big data and financial-related components will be revealed in an exploratory literature review of secondary data sources. Since big data in the financial field is an extremely new concept, future research directions will be pointed out at the end of this study.",10.1186/s40537-020-00291-z
75c364909914f17791837ec88090262aa6656d3e,Big data in IBD: big progress for clinical practice,2020,"IBD is a complex multifactorial inflammatory disease of the gut driven by extrinsic and intrinsic factors, including host genetics, the immune system, environmental factors and the gut microbiome. Technological advancements such as next-generation sequencing, high-throughput omics data generation and molecular networks have catalysed IBD research. The advent of artificial intelligence, in particular, machine learning, and systems biology has opened the avenue for the efficient integration and interpretation of big datasets for discovering clinically translatable knowledge. In this narrative review, we discuss how big data integration and machine learning have been applied to translational IBD research. Approaches such as machine learning may enable patient stratification, prediction of disease progression and therapy responses for fine-tuning treatment options with positive impacts on cost, health and safety. We also outline the challenges and opportunities presented by machine learning and big data in clinical IBD research.",10.1136/gutjnl-2019-320065
18d87bff073687c025f9bd23ab2dfb20d5f72a66,BIM Big Data Storage in WebVRGIS,2020,"In the context of big data and the Internet of Things, with the advancement of geospatial data acquisition and retrieval, the volume of available geospatial data is increasing every minute. Thus, new data-management architecture is needed. We proposed a building information model (BIM) big data-storage-management solution with hybrid storage architecture based on web virtual reality geographical information system (WebVRGIS). BIM is associated with the integration of spatial and semantic information on the various stages of urban building. In this paper, based on the spatial distribution characteristics of BIM geospatial big data, a data storage and management model is proposed for BIM geospatial big data management. The architecture primarily includes Not only Structured Query Language (NoSQL) database and distributed peer-to-peer storage. The evaluation of the proposed storage method is conducted on the same software platform as our previous research about WebVR. The experimental results show that the hybrid storage architecture proposed in this research has a lower response time compared to the traditional relational database in geospatial big data searches. The integration and fusion of BIM big data in WebVRGIS realizes a revolutionary transformation of city information management during a full lifecycle. The system also has great promise for the storage of other geospatial big data, such as traffic data.",10.1109/TII.2019.2916689
ba9b6f805feb62c978d384211f910790643a023e,Big data monetization throughout Big Data Value Chain: a comprehensive review,2020,"Value Chain has been considered as a key model for managing efficiently value creation processes within organizations. However, with the digitization of the end-to-end processes which began to adopt data as a main source of value, traditional value chain models have become outdated. For this, researchers have developed new value chain models, called Data Value Chains, to carry out data driven organizations. Thereafter, new data value chains called Big Data Value chain have emerged with the emergence of Big Data in order to face new data-related challenges such as high volume, velocity, and variety. These Big Data Value Chains describe the data flow within organizations which rely on Big Data to extract valuable insights. It is a set of ordered steps using Big Data Analytics tools and mainly built for going from data generation to knowledge creation. The advances in Big Data and Big Data Value Chain, using clear processes for aggregation and exploitation of data, have given rise to what is called data monetization. Data monetization concept consists of using data from an organization to generate profit. It may be selling the data directly for cash, or relying on that data to create value indirectly. It is important to mention that the concept of monetizing data is not as new as it looks, but with the era of Big Data and Big Data Value Chain it is becoming attractive. The aim of this paper is to provide a comprehensive review of value creation, data value, and Big Data value chains with their different steps. This literature has led us to construct an end-to-end exhaustive BDVC that regroup most of the addressed phases. Furthermore, we present a possible evolution of that generic BDVC to support Big Data Monetization. For this, we discuss different approaches that enable data monetization throughout data value chains. Finally, we highlight the need to adopt specific data monetization models to suit big data specificities.",10.1186/s40537-019-0281-5
697f2f3598057cd17cff7749d768cae0993c6727,A Framework for Pandemic Prediction Using Big Data Analytics,2021,,10.1016/j.bdr.2021.100190
e7c8fcbc24c73a576339e5f34f9f23f5ea732b3b,Creating Strategic Business Value from Big Data Analytics: A Research Framework,2018,"Abstract Despite the publicity regarding big data and analytics (BDA), the success rate of these projects and strategic value created from them are unclear. Most literature on BDA focuses on how it can be used to enhance tactical organizational capabilities, but very few studies examine its impact on organizational value. Further, we see limited framing of how BDA can create strategic value for the organization. After all, the ultimate success of any BDA project lies in realizing strategic business value, which gives firms a competitive advantage. In this study, we describe the value proposition of BDA by delineating its components. We offer a framing of BDA value by extending existing frameworks of information technology value, then illustrate the framework through BDA applications in practice. The framework is then discussed in terms of its ability to study constructs and relationships that focus on BDA value creation and realization. We also present a problem-oriented view of the framework—where problems in BDA components can give rise to targeted research questions and areas for future study. The framing in this study could help develop a significant research agenda for BDA that can better target research and practice based on effective use of data resources.",10.1080/07421222.2018.1451951
99f06e88e76f1af51d08d7adfb26d758ebc6acab,Advanced data analytics for enhancing building performances: From data-driven to big data-driven approaches,2020,,10.1007/s12273-020-0723-1
8adb47deeef943c2c1bae41f9498a382fb818a16,"Big data in education: a state of the art, limitations, and future research directions",2020,"Big data is an essential aspect of innovation which has recently gained major attention from both academics and practitioners. Considering the importance of the education sector, the current tendency is moving towards examining the role of big data in this sector. So far, many studies have been conducted to comprehend the application of big data in different fields for various purposes. However, a comprehensive review is still lacking in big data in education. Thus, this study aims to conduct a systematic review on big data in education in order to explore the trends, classify the research themes, and highlight the limitations and provide possible future directions in the domain. Following a systematic review procedure, 40 primary studies published from 2014 to 2019 were utilized and related information extracted. The findings showed that there is an increase in the number of studies that address big data in education during the last 2 years. It has been found that the current studies covered four main research themes under big data in education, mainly, learner’s behavior and performance, modelling and educational data warehouse, improvement in the educational system, and integration of big data into the curriculum. Most of the big data educational researches have focused on learner’s behavior and performances. Moreover, this study highlights research limitations and portrays the future directions. This study provides a guideline for future studies and highlights new insights and directions for the successful utilization of big data in education.",10.1186/s41239-020-00223-0
9810bcaf5ac1792e6a2738a86f85ce270d448040,Flexible and durable wood-based triboelectric nanogenerators for self-powered sensing in athletic big data analytics,2019,"In the new era of internet of things, big data collection and analysis based on widely distributed intelligent sensing technology is particularly important. Here, we report a flexible and durable wood-based triboelectric nanogenerator for self-powered sensing in athletic big data analytics. Based on a simple and effective strategy, natural wood can be converted into a high-performance triboelectric material with excellent mechanical properties, such as 7.5-fold enhancement in strength, superior flexibility, wear resistance and processability. The electrical output performance is also enhanced by more than 70% compared with natural wood. A self-powered falling point distribution statistical system and an edge ball judgement system are further developed to provide training guidance and real-time competition assistance for both athletes and referees. This work can not only expand the application area of the self-powered system to smart sport monitoring and assisting, but also promote the development of big data analytics in intelligent sports industry. Intelligent sensing technologies gain interest for the internet of things and applications that require collection and analysis of big data. Here the authors report a flexible and durable wood-based triboelectric nanogenerator for self-powered sensing in athletic big data analytics.",10.1038/s41467-019-13166-6
cc1e82125f7f8636b25ccdcdb63e8f812add7f87,A Big Data Enabled Channel Model for 5G Wireless Communication Systems,2020,"The standardization process of the fifth generation (5G) wireless communications has recently been accelerated and the first commercial 5G services would be provided as early as in 2018. The increasing of enormous smartphones, new complex scenarios, large frequency bands, massive antenna elements, and dense small cells will generate big datasets and bring 5G communications to the era of big data. This paper investigates various applications of big data analytics, especially machine learning algorithms in wireless communications and channel modeling. We propose a big data and machine learning enabled wireless channel model framework. The proposed channel model is based on artificial neural networks (ANNs), including feed-forward neural network (FNN) and radial basis function neural network (RBF-NN). The input parameters are transmitter (Tx) and receiver (Rx) coordinates, Tx–Rx distance, and carrier frequency, while the output parameters are channel statistical properties, including the received power, root mean square (RMS) delay spread (DS), and RMS angle spreads (ASs). Datasets used to train and test the ANNs are collected from both real channel measurements and a geometry based stochastic model (GBSM). Simulation results show good performance and indicate that machine learning algorithms can be powerful analytical tools for future measurement-based wireless channel modeling.",10.1109/TBDATA.2018.2884489
8d2142cf2b9ffdbdaf13473c39a6b1bd737d12ba,Deep learning and big data technologies for IoT security,2020,,10.1016/j.comcom.2020.01.016
0026ea8a0fd31bdc959a4b9ed4d449f3015be8d1,Big Data and Its Applications in Smart Real Estate and the Disaster Management Life Cycle: A Systematic Analysis,2020,"Big data is the concept of enormous amounts of data being generated daily in different fields due to the increased use of technology and internet sources. Despite the various advancements and the hopes of better understanding, big data management and analysis remain a challenge, calling for more rigorous and detailed research, as well as the identifications of methods and ways in which big data could be tackled and put to good use. The existing research lacks in discussing and evaluating the pertinent tools and technologies to analyze big data in an efficient manner which calls for a comprehensive and holistic analysis of the published articles to summarize the concept of big data and see field-specific applications. To address this gap and keep a recent focus, research articles published in last decade, belonging to top-tier and high-impact journals, were retrieved using the search engines of Google Scholar, Scopus, and Web of Science that were narrowed down to a set of 139 relevant research articles. Different analyses were conducted on the retrieved papers including bibliometric analysis, keywords analysis, big data search trends, and authors’ names, countries, and affiliated institutes contributing the most to the field of big data. The comparative analyses show that, conceptually, big data lies at the intersection of the storage, statistics, technology, and research fields and emerged as an amalgam of these four fields with interlinked aspects such as data hosting and computing, data management, data refining, data patterns, and machine learning. The results further show that major characteristics of big data can be summarized using the seven Vs, which include variety, volume, variability, value, visualization, veracity, and velocity. Furthermore, the existing methods for big data analysis, their shortcomings, and the possible directions were also explored that could be taken for harnessing technology to ensure data analysis tools could be upgraded to be fast and efficient. The major challenges in handling big data include efficient storage, retrieval, analysis, and visualization of the large heterogeneous data, which can be tackled through authentication such as Kerberos and encrypted files, logging of attacks, secure communication through Secure Sockets Layer (SSL) and Transport Layer Security (TLS), data imputation, building learning models, dividing computations into sub-tasks, checkpoint applications for recursive tasks, and using Solid State Drives (SDD) and Phase Change Material (PCM) for storage. In terms of frameworks for big data management, two frameworks exist including Hadoop and Apache Spark, which must be used simultaneously to capture the holistic essence of the data and make the analyses meaningful, swift, and speedy. Further field-specific applications of big data in two promising and integrated fields, i.e., smart real estate and disaster management, were investigated, and a framework for field-specific applications, as well as a merger of the two areas through big data, was highlighted. The proposed frameworks show that big data can tackle the ever-present issues of customer regrets related to poor quality of information or lack of information in smart real estate to increase the customer satisfaction using an intermediate organization that can process and keep a check on the data being provided to the customers by the sellers and real estate managers. Similarly, for disaster and its risk management, data from social media, drones, multimedia, and search engines can be used to tackle natural disasters such as floods, bushfires, and earthquakes, as well as plan emergency responses. In addition, a merger framework for smart real estate and disaster risk management show that big data generated from the smart real estate in the form of occupant data, facilities management, and building integration and maintenance can be shared with the disaster risk management and emergency response teams to help prevent, prepare, respond to, or recover from the disasters.",10.3390/bdcc4020004
b0ee814c7a3eed260c9913861329c9f73e880d00,DEA under big data: data enabled analytics and network data envelopment analysis,2020,,10.1007/s10479-020-03668-8
7f5cd5b1340ac06ea38bd05373c30136a6f4c1ca,The value of Big Data in government: The case of ‘smart cities’,2020,"The emergence of Big Data has added a new aspect to conceptualizing the use of digital technologies in the delivery of public services and for realizing digital governance. This article explores, via the ‘value-chain’ approach, the evolution of digital governance research, and aligns it with current developments associated with data analytics, often referred to as ‘Big Data’. In many ways, the current discourse around Big Data reiterates and repeats established commentaries within the eGovernment research community. This body of knowledge provides an opportunity to reflect on the ‘promise’ of Big Data, both in relation to service delivery and policy formulation. This includes, issues associated with the quality and reliability of data, from mixing public and private sector data, issues associated with the ownership of raw and manipulated data, and ethical issues concerning surveillance and privacy. These insights and the issues raised help assess the value of Big Data in government and smart city environments.",10.1177/2053951720912775
971c35bcab25fbf4fd4bb6e128cf2586f0ab1d67,Manufacturing big data ecosystem: A systematic literature review,2020,,10.1016/j.rcim.2019.101861
521e5c337be51b8f8fdb858580bb46a0545ab1f9,When Gaussian Process Meets Big Data: A Review of Scalable GPs,2018,"The vast quantity of information brought by big data as well as the evolving computer hardware encourages success stories in the machine learning community. In the meanwhile, it poses challenges for the Gaussian process regression (GPR), a well-known nonparametric, and interpretable Bayesian model, which suffers from cubic complexity to data size. To improve the scalability while retaining desirable prediction quality, a variety of scalable GPs have been presented. However, they have not yet been comprehensively reviewed and analyzed to be well understood by both academia and industry. The review of scalable GPs in the GP community is timely and important due to the explosion of data size. To this end, this article is devoted to reviewing state-of-the-art scalable GPs involving two main categories: global approximations that distillate the entire data and local approximations that divide the data for subspace learning. Particularly, for global approximations, we mainly focus on sparse approximations comprising prior approximations that modify the prior but perform exact inference, posterior approximations that retain exact prior but perform approximate inference, and structured sparse approximations that exploit specific structures in kernel matrix; for local approximations, we highlight the mixture/product of experts that conducts model averaging from multiple local experts to boost predictions. To present a complete review, recent advances for improving the scalability and capability of scalable GPs are reviewed. Finally, the extensions and open issues of scalable GPs in various scenarios are reviewed and discussed to inspire novel ideas for future research avenues.",10.1109/TNNLS.2019.2957109
1f8a23697562b001082b147779b5eaefd3513d0a,Human migration: the big data perspective,2020,"How can big data help to understand the migration phenomenon? In this paper, we try to answer this question through an analysis of various phases of migration, comparing traditional and novel data sources and models at each phase. We concentrate on three phases of migration, at each phase describing the state of the art and recent developments and ideas. The first phase includes the journey, and we study migration flows and stocks, providing examples where big data can have an impact. The second phase discusses the stay, i.e. migrant integration in the destination country. We explore various data sets and models that can be used to quantify and understand migrant integration, with the final aim of providing the basis for the construction of a novel multi-level integration index. The last phase is related to the effects of migration on the source countries and the return of migrants.",10.1007/s41060-020-00213-5
ed9e7821b3e51c7e59183300d6c8cf90c8de0f26,COVID-19 is spatial: Ensuring that mobile Big Data is used for social good,2020,"The mobility restrictions related to COVID-19 pandemic have resulted in the biggest disruption to individual mobilities in modern times. The crisis is clearly spatial in nature, and examining the geographical aspect is important in understanding the broad implications of the pandemic. The avalanche of mobile Big Data makes it possible to study the spatial effects of the crisis with spatiotemporal detail at the national and global scales. However, the current crisis also highlights serious limitations in the readiness to take the advantage of mobile Big Data for social good, both within and beyond the interests of health sector. We propose two strategical pathways for the future use of mobile Big Data for societal impact assessment, addressing access to both raw mobile Big Data as well as aggregated data products. Both pathways require careful considerations of privacy issues, harmonized and transparent methodologies, and attention to the representativeness, reliability and continuity of data. The goal is to be better prepared to use mobile Big Data in future crises.",10.1177/2053951720952088
d63b884d5ebc739f6e1bdf861fa9276260781404,Deep Learning for IoT Big Data and Streaming Analytics: A Survey,2017,"In the era of the Internet of Things (IoT), an enormous amount of sensing devices collect and/or generate various sensory data over time for a wide range of fields and applications. Based on the nature of the application, these devices will result in big or fast/real-time data streams. Applying analytics over such data streams to discover new information, predict future insights, and make control decisions is a crucial process that makes IoT a worthy paradigm for businesses and a quality-of-life improving technology. In this paper, we provide a thorough overview on using a class of advanced machine learning techniques, namely deep learning (DL), to facilitate the analytics and learning in the IoT domain. We start by articulating IoT data characteristics and identifying two major treatments for IoT data from a machine learning perspective, namely IoT big data analytics and IoT streaming data analytics. We also discuss why DL is a promising approach to achieve the desired analytics in these types of data and applications. The potential of using emerging DL techniques for IoT data analytics are then discussed, and its promises and challenges are introduced. We present a comprehensive background on different DL architectures and algorithms. We also analyze and summarize major reported research attempts that leveraged DL in the IoT domain. The smart IoT devices that have incorporated DL in their intelligence background are also discussed. DL implementation approaches on the fog and cloud centers in support of IoT applications are also surveyed. Finally, we shed light on some challenges and potential directions for future research. At the end of each section, we highlight the lessons learned based on our experiments and review of the recent literature.",10.1109/COMST.2018.2844341
243d553f1fc5b7ee8cfb6f49629f3a0a32b2c5c5,Big Data Analytics in Operations Management,2018,"Big data analytics is critical in modern operations management (OM). In this study, we first explore the existing big data‐related analytics techniques, and identify their strengths, weaknesses as well as major functionalities. We then discuss various big data analytics strategies to overcome the respective computational and data challenges. After that, we examine the literature and reveal how different types of big data methods (techniques, strategies, and architectures) can be applied to different OM topical areas, namely forecasting, inventory management, revenue management and marketing, transportation management, supply chain management, and risk analysis. We also investigate via case studies the real‐world applications of big data analytics in top branded enterprises. Finally, we conclude the study with a discussion of future research.",10.1111/poms.12838
d65d64c3f6ea322d9e85138fe5c8e85acbf661e3,A Bibliometric Analysis and Visualization of Medical Big Data Research,2018,"With the rapid development of “Internet plus”, medical care has entered the era of big data. However, there is little research on medical big data (MBD) from the perspectives of bibliometrics and visualization. The substantive research on the basic aspects of MBD itself is also rare. This study aims to explore the current status of medical big data through visualization analysis on the journal papers related to MBD. We analyze a total of 988 references which were downloaded from the Science Citation Index Expanded and the Social Science Citation Index databases from Web of Science and the time span was defined as “all years”. The GraphPad Prism 5, VOSviewer and CiteSpace softwares are used for analysis. Many results concerning the annual trends, the top players in terms of journal and institute levels, the citations and H-index in terms of country level, the keywords distribution, the highly cited papers, the co-authorship status and the most influential journals and authors are presented in this paper. This study points out the development status and trends on MBD. It can help people in the medical profession to get comprehensive understanding on the state of the art of MBD. It also has reference values for the research and application of the MBD visualization methods.",10.3390/SU10010166
d99e88d3c1821857ca6945470698351925f9737f,A survey on addressing high-class imbalance in big data,2018,"In a majority–minority classification problem, class imbalance in the dataset(s) can dramatically skew the performance of classifiers, introducing a prediction bias for the majority class. Assuming the positive (minority) class is the group of interest and the given application domain dictates that a false negative is much costlier than a false positive, a negative (majority) class prediction bias could have adverse consequences. With big data, the mitigation of class imbalance poses an even greater challenge because of the varied and complex structure of the relatively much larger datasets. This paper provides a large survey of published studies within the last 8 years, focusing on high-class imbalance (i.e., a majority-to-minority class ratio between 100:1 and 10,000:1) in big data in order to assess the state-of-the-art in addressing adverse effects due to class imbalance. In this paper, two techniques are covered which include Data-Level (e.g., data sampling) and Algorithm-Level (e.g., cost-sensitive and hybrid/ensemble) Methods. Data sampling methods are popular in addressing class imbalance, with Random Over-Sampling methods generally showing better overall results. At the Algorithm-Level, there are some outstanding performers. Yet, in the published studies, there are inconsistent and conflicting results, coupled with a limited scope in evaluated techniques, indicating the need for more comprehensive, comparative studies.",10.1186/s40537-018-0151-6
6eefbb79f98e902e9d2efa057bfea174843bf3dc,Analysis of healthcare big data,2020,,10.1016/j.future.2020.03.039
8e6d0ed32aaa5e3d7c598d5a2ace76eab8485801,"On big data, artificial intelligence and smart cities",2019,,10.1016/J.CITIES.2019.01.032
91fc647899f801c9d351349ce73779918f90a713,Big data and machine learning algorithms for health-care delivery.,2019,,10.1016/S1470-2045(19)30149-4
ecef432e7f6c9f431d5b34706a8de1fdebec46f9,From Big Data to Precision Medicine,2019,"For over a decade the term “Big data” has been used to describe the rapid increase in volume, variety and velocity of information available, not just in medical research but in almost every aspect of our lives. As scientists, we now have the capacity to rapidly generate, store and analyse data that, only a few years ago, would have taken many years to compile. However, “Big data” no longer means what it once did. The term has expanded and now refers not to just large data volume, but to our increasing ability to analyse and interpret those data. Tautologies such as “data analytics” and “data science” have emerged to describe approaches to the volume of available information as it grows ever larger. New methods dedicated to improving data collection, storage, cleaning, processing and interpretation continue to be developed, although not always by, or for, medical researchers. Exploiting new tools to extract meaning from large volume information has the potential to drive real change in clinical practice, from personalized therapy and intelligent drug design to population screening and electronic health record mining. As ever, where new technology promises “Big Advances,” significant challenges remain. Here we discuss both the opportunities and challenges posed to biomedical research by our increasing ability to tackle large datasets. Important challenges include the need for standardization of data content, format, and clinical definitions, a heightened need for collaborative networks with sharing of both data and expertise and, perhaps most importantly, a need to reconsider how and when analytic methodology is taught to medical researchers. We also set “Big data” analytics in context: recent advances may appear to promise a revolution, sweeping away conventional approaches to medical science. However, their real promise lies in their synergy with, not replacement of, classical hypothesis-driven methods. The generation of novel, data-driven hypotheses based on interpretable models will always require stringent validation and experimental testing. Thus, hypothesis-generating research founded on large datasets adds to, rather than replaces, traditional hypothesis driven science. Each can benefit from the other and it is through using both that we can improve clinical practice.",10.3389/fmed.2019.00034
